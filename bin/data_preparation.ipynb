{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyBigWig\n",
    "import pybedtools # if on M1 chip mac while installing bedtools make sure conda config --env --set subdir osx-64 (i was on arm-64 and it could not find bedtools!!!) also install rosetta2 /usr/sbin/softwareupdate --install-rosetta --agree-to-license\n",
    "from pybedtools import BedTool\n",
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "# change parent directory to use tools\n",
    "\n",
    "from pathlib import Path\n",
    "os.chdir(Path.cwd().parent)\n",
    "from introgression_tools import tools\n",
    "\n",
    "HOMEDIR='/wynton/home/capra/ychen39/'\n",
    "BEDTOOLSDIR = '/wynton/home/cbi/shared/software/CBI/bedtools2-2.31.1/bin'\n",
    "# set bedtools path\n",
    "pybedtools.helpers.set_bedtools_path(path=f'{BEDTOOLSDIR}')\n",
    "\n",
    "methods_dirs = ['chen_2020', 'yuan_2021', 'durvasula_2019', 'steinruecken_2018', 'skov_2020', 'schaefer_2021', 'hubisz_2020', 'browning_2018', 'vernot_2016', 'sankararaman_2014', 'sankararaman_2016_1', 'sankararaman_2016_2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generating standardized variant-level files across introgression methods\n",
    "SNPs are only available for Sankararaman14, Sankararaman16 (1 and 2), S*, Sprime, and Skov20.\n",
    "\n",
    "We will also make .bed files for each ancestral group.\n",
    "\n",
    "Here, we use the introgression_tools package to generate standardized variant-level data. Vernot and CRF variant-level introgression files were generated from Bash scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sankararaman 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39/introgression_methods/cleaned/introgression_tools/sankararaman_2014/’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEU\n",
      "chr-10.thresh-90.length-0.00.gz\n",
      "chr-11.thresh-90.length-0.00.gz\n",
      "chr-12.thresh-90.length-0.00.gz\n",
      "chr-13.thresh-90.length-0.00.gz\n",
      "chr-14.thresh-90.length-0.00.gz\n",
      "chr-15.thresh-90.length-0.00.gz\n",
      "chr-16.thresh-90.length-0.00.gz\n",
      "chr-17.thresh-90.length-0.00.gz\n",
      "chr-18.thresh-90.length-0.00.gz\n",
      "chr-19.thresh-90.length-0.00.gz\n",
      "chr-1.thresh-90.length-0.00.gz\n",
      "chr-20.thresh-90.length-0.00.gz\n",
      "chr-21.thresh-90.length-0.00.gz\n",
      "chr-22.thresh-90.length-0.00.gz\n",
      "chr-2.thresh-90.length-0.00.gz\n",
      "chr-3.thresh-90.length-0.00.gz\n",
      "chr-4.thresh-90.length-0.00.gz\n",
      "chr-5.thresh-90.length-0.00.gz\n",
      "chr-6.thresh-90.length-0.00.gz\n",
      "chr-7.thresh-90.length-0.00.gz\n",
      "chr-8.thresh-90.length-0.00.gz\n",
      "chr-9.thresh-90.length-0.00.gz\n",
      "chr-X.thresh-90.length-0.00.gz\n",
      "FIN\n",
      "chr-10.thresh-90.length-0.00.gz\n",
      "chr-11.thresh-90.length-0.00.gz\n",
      "chr-12.thresh-90.length-0.00.gz\n",
      "chr-13.thresh-90.length-0.00.gz\n",
      "chr-14.thresh-90.length-0.00.gz\n",
      "chr-15.thresh-90.length-0.00.gz\n",
      "chr-16.thresh-90.length-0.00.gz\n",
      "chr-17.thresh-90.length-0.00.gz\n",
      "chr-18.thresh-90.length-0.00.gz\n",
      "chr-19.thresh-90.length-0.00.gz\n",
      "chr-1.thresh-90.length-0.00.gz\n",
      "chr-20.thresh-90.length-0.00.gz\n",
      "chr-21.thresh-90.length-0.00.gz\n",
      "chr-22.thresh-90.length-0.00.gz\n",
      "chr-2.thresh-90.length-0.00.gz\n",
      "chr-3.thresh-90.length-0.00.gz\n",
      "chr-4.thresh-90.length-0.00.gz\n",
      "chr-5.thresh-90.length-0.00.gz\n",
      "chr-6.thresh-90.length-0.00.gz\n",
      "chr-7.thresh-90.length-0.00.gz\n",
      "chr-8.thresh-90.length-0.00.gz\n",
      "chr-9.thresh-90.length-0.00.gz\n",
      "chr-X.thresh-90.length-0.00.gz\n",
      "GBR\n",
      "chr-10.thresh-90.length-0.00.gz\n",
      "chr-11.thresh-90.length-0.00.gz\n",
      "chr-12.thresh-90.length-0.00.gz\n",
      "chr-13.thresh-90.length-0.00.gz\n",
      "chr-14.thresh-90.length-0.00.gz\n",
      "chr-15.thresh-90.length-0.00.gz\n",
      "chr-16.thresh-90.length-0.00.gz\n",
      "chr-17.thresh-90.length-0.00.gz\n",
      "chr-18.thresh-90.length-0.00.gz\n",
      "chr-19.thresh-90.length-0.00.gz\n",
      "chr-1.thresh-90.length-0.00.gz\n",
      "chr-20.thresh-90.length-0.00.gz\n",
      "chr-21.thresh-90.length-0.00.gz\n",
      "chr-22.thresh-90.length-0.00.gz\n",
      "chr-2.thresh-90.length-0.00.gz\n",
      "chr-3.thresh-90.length-0.00.gz\n",
      "chr-4.thresh-90.length-0.00.gz\n",
      "chr-5.thresh-90.length-0.00.gz\n",
      "chr-6.thresh-90.length-0.00.gz\n",
      "chr-7.thresh-90.length-0.00.gz\n",
      "chr-8.thresh-90.length-0.00.gz\n",
      "chr-9.thresh-90.length-0.00.gz\n",
      "chr-X.thresh-90.length-0.00.gz\n",
      "IBS\n",
      "chr-10.thresh-90.length-0.00.gz\n",
      "chr-11.thresh-90.length-0.00.gz\n",
      "chr-12.thresh-90.length-0.00.gz\n",
      "chr-13.thresh-90.length-0.00.gz\n",
      "chr-14.thresh-90.length-0.00.gz\n",
      "chr-15.thresh-90.length-0.00.gz\n",
      "chr-16.thresh-90.length-0.00.gz\n",
      "chr-17.thresh-90.length-0.00.gz\n",
      "chr-18.thresh-90.length-0.00.gz\n",
      "chr-19.thresh-90.length-0.00.gz\n",
      "chr-1.thresh-90.length-0.00.gz\n",
      "chr-20.thresh-90.length-0.00.gz\n",
      "chr-21.thresh-90.length-0.00.gz\n",
      "chr-22.thresh-90.length-0.00.gz\n",
      "chr-2.thresh-90.length-0.00.gz\n",
      "chr-3.thresh-90.length-0.00.gz\n",
      "chr-4.thresh-90.length-0.00.gz\n",
      "chr-5.thresh-90.length-0.00.gz\n",
      "chr-6.thresh-90.length-0.00.gz\n",
      "chr-7.thresh-90.length-0.00.gz\n",
      "chr-8.thresh-90.length-0.00.gz\n",
      "chr-9.thresh-90.length-0.00.gz\n",
      "chr-X.thresh-90.length-0.00.gz\n",
      "TSI\n",
      "chr-10.thresh-90.length-0.00.gz\n",
      "chr-11.thresh-90.length-0.00.gz\n",
      "chr-12.thresh-90.length-0.00.gz\n",
      "chr-13.thresh-90.length-0.00.gz\n",
      "chr-14.thresh-90.length-0.00.gz\n",
      "chr-15.thresh-90.length-0.00.gz\n",
      "chr-16.thresh-90.length-0.00.gz\n",
      "chr-17.thresh-90.length-0.00.gz\n",
      "chr-18.thresh-90.length-0.00.gz\n",
      "chr-19.thresh-90.length-0.00.gz\n",
      "chr-1.thresh-90.length-0.00.gz\n",
      "chr-20.thresh-90.length-0.00.gz\n",
      "chr-21.thresh-90.length-0.00.gz\n",
      "chr-22.thresh-90.length-0.00.gz\n",
      "chr-2.thresh-90.length-0.00.gz\n",
      "chr-3.thresh-90.length-0.00.gz\n",
      "chr-4.thresh-90.length-0.00.gz\n",
      "chr-5.thresh-90.length-0.00.gz\n",
      "chr-6.thresh-90.length-0.00.gz\n",
      "chr-7.thresh-90.length-0.00.gz\n",
      "chr-8.thresh-90.length-0.00.gz\n",
      "chr-9.thresh-90.length-0.00.gz\n",
      "chr-X.thresh-90.length-0.00.gz\n",
      "CHB\n",
      "chr-10.thresh-90.length-0.00.gz\n",
      "chr-11.thresh-90.length-0.00.gz\n",
      "chr-12.thresh-90.length-0.00.gz\n",
      "chr-13.thresh-90.length-0.00.gz\n",
      "chr-14.thresh-90.length-0.00.gz\n",
      "chr-15.thresh-90.length-0.00.gz\n",
      "chr-16.thresh-90.length-0.00.gz\n",
      "chr-17.thresh-90.length-0.00.gz\n",
      "chr-18.thresh-90.length-0.00.gz\n",
      "chr-19.thresh-90.length-0.00.gz\n",
      "chr-1.thresh-90.length-0.00.gz\n",
      "chr-20.thresh-90.length-0.00.gz\n",
      "chr-21.thresh-90.length-0.00.gz\n",
      "chr-22.thresh-90.length-0.00.gz\n",
      "chr-2.thresh-90.length-0.00.gz\n",
      "chr-3.thresh-90.length-0.00.gz\n",
      "chr-4.thresh-90.length-0.00.gz\n",
      "chr-5.thresh-90.length-0.00.gz\n",
      "chr-6.thresh-90.length-0.00.gz\n",
      "chr-7.thresh-90.length-0.00.gz\n",
      "chr-8.thresh-90.length-0.00.gz\n",
      "chr-9.thresh-90.length-0.00.gz\n",
      "chr-X.thresh-90.length-0.00.gz\n",
      "CHS\n",
      "chr-10.thresh-90.length-0.00.gz\n",
      "chr-11.thresh-90.length-0.00.gz\n",
      "chr-12.thresh-90.length-0.00.gz\n",
      "chr-13.thresh-90.length-0.00.gz\n",
      "chr-14.thresh-90.length-0.00.gz\n",
      "chr-15.thresh-90.length-0.00.gz\n",
      "chr-16.thresh-90.length-0.00.gz\n",
      "chr-17.thresh-90.length-0.00.gz\n",
      "chr-18.thresh-90.length-0.00.gz\n",
      "chr-19.thresh-90.length-0.00.gz\n",
      "chr-1.thresh-90.length-0.00.gz\n",
      "chr-20.thresh-90.length-0.00.gz\n",
      "chr-21.thresh-90.length-0.00.gz\n",
      "chr-22.thresh-90.length-0.00.gz\n",
      "chr-2.thresh-90.length-0.00.gz\n",
      "chr-3.thresh-90.length-0.00.gz\n",
      "chr-4.thresh-90.length-0.00.gz\n",
      "chr-5.thresh-90.length-0.00.gz\n",
      "chr-6.thresh-90.length-0.00.gz\n",
      "chr-7.thresh-90.length-0.00.gz\n",
      "chr-8.thresh-90.length-0.00.gz\n",
      "chr-9.thresh-90.length-0.00.gz\n",
      "chr-X.thresh-90.length-0.00.gz\n",
      "JPT\n",
      "chr-10.thresh-90.length-0.00.gz\n",
      "chr-11.thresh-90.length-0.00.gz\n",
      "chr-12.thresh-90.length-0.00.gz\n",
      "chr-13.thresh-90.length-0.00.gz\n",
      "chr-14.thresh-90.length-0.00.gz\n",
      "chr-15.thresh-90.length-0.00.gz\n",
      "chr-16.thresh-90.length-0.00.gz\n",
      "chr-17.thresh-90.length-0.00.gz\n",
      "chr-18.thresh-90.length-0.00.gz\n",
      "chr-19.thresh-90.length-0.00.gz\n",
      "chr-1.thresh-90.length-0.00.gz\n",
      "chr-20.thresh-90.length-0.00.gz\n",
      "chr-21.thresh-90.length-0.00.gz\n",
      "chr-22.thresh-90.length-0.00.gz\n",
      "chr-2.thresh-90.length-0.00.gz\n",
      "chr-3.thresh-90.length-0.00.gz\n",
      "chr-4.thresh-90.length-0.00.gz\n",
      "chr-5.thresh-90.length-0.00.gz\n",
      "chr-6.thresh-90.length-0.00.gz\n",
      "chr-7.thresh-90.length-0.00.gz\n",
      "chr-8.thresh-90.length-0.00.gz\n",
      "chr-9.thresh-90.length-0.00.gz\n",
      "chr-X.thresh-90.length-0.00.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# set up directories\n",
    "INPUTDIR=\"/wynton/home/capra/ychen39/introgression_methods/data/sankararaman_2014_copy/sankararaman_2014\"\n",
    "OUTPUTDIR=\"/wynton/home/capra/ychen39/introgression_methods/cleaned/introgression_tools/sankararaman_2014/\"\n",
    "\n",
    "mkdir $OUTPUTDIR\n",
    "\n",
    "# for each population file\n",
    "# add column names\n",
    "echo -e \"Chromosome\\tEnd\\tAlleleScore\\tHaplotypeScore\\tPopulation\" > $OUTPUTDIR/neanderthal_introgressed_variants_pops.bed\n",
    "# add data from each population file\n",
    "for pop in $(cat $INPUTDIR/pops.EUR-ASN); do\n",
    "    echo $pop\n",
    "    cd $INPUTDIR/\"$pop\".hapmap/summaries/\n",
    "    FILES=`ls --color=never *.gz`\n",
    "    for file in $FILES; do\n",
    "        echo $file\n",
    "        # keep entries if their nean probabilities (column 11) are greater than 0.9\n",
    "        gunzip -c $file | awk -v pop=\"$pop\" '{ if ($10+0 > 0.9) print $2 \"\\t\" $4 \"\\t\" $10 \"\\t\" $11 \"\\t\" pop}' >> $OUTPUTDIR/pops.bed\n",
    "    done\n",
    "done\n",
    "# remove extra file lines except ones starting with numbers (chromosomes), X (for chromosome X), and C (header line)\n",
    "grep '^[0-9 | X | C]' $OUTPUTDIR/pops.bed > $OUTPUTDIR/neanderthal_introgressed_variants_pops.bed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tools.save_bed_only(full_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/neanderthal_introgressed_variants_pops.bed', \n",
    "              output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/neanderthal_introgressed_variants.bg', \n",
    "              prefix=False, \n",
    "              save_with_header=False,\n",
    "              colnames=['Chromosome', 'End', 'AlleleScore', 'HaplotypeScore', 'Ancestry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with chr prefix\n",
    "tools.save_bed_only(full_file_path=f'{HOMEDIR}/introgression_methods/cleaned/sankararaman_2014_copy/neanderthal_introgressed_variants_pops.bed', \n",
    "              output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/neanderthal_introgressed_variants_chr_prefix.bg', \n",
    "              prefix=True, \n",
    "              save_with_header=False,\n",
    "              colnames=['Chromosome', 'End', 'AlleleScore', 'HaplotypeScore', 'Ancestry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make EUR variant files\n",
    "sankararaman_2014 = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/neanderthal_introgressed_variants_pops.bed', \n",
    "                            sep='\\t', low_memory=False)\n",
    "\n",
    "sankararaman_2014['Ancestry'] = sankararaman_2014['Population'].apply(tools.map_status)\n",
    "\n",
    "sankararaman_2014_EUR=sankararaman_2014[sankararaman_2014['Ancestry']=='EUR']\n",
    "\n",
    "tools.save_bed_only(df=sankararaman_2014_EUR, \n",
    "         output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/neanderthal_introgressed_variants_EUR_chr_prefix.bed', prefix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sankararaman 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sankararaman 2016 - 1\n",
    "# Repeat for first directory\n",
    "\n",
    "# set up directories\n",
    "INPUTDIR=\"/wynton/home/capra/ychen39/introgression_methods/data/sankararaman_2016/summaries/1/neandertal\"\n",
    "OUTPUTDIR=\"/wynton/home/capra/ychen39/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1\"\n",
    "\n",
    "mkdir $OUTPUTDIR\n",
    "\n",
    "# add column names\n",
    "echo -e \"Chromosome\\tEnd\\tAlleleScore\\tHaplotypeScore\\tPopulation\" > $OUTPUTDIR/pops.bed\n",
    "# add data from each population file\n",
    "for pop in $(cat $INPUTDIR/../../pops); do\n",
    "    cd $INPUTDIR/\"$pop\"/summaries/\n",
    "    FILES=`ls --color=never pred*`\n",
    "    for file in $FILES; do\n",
    "        echo $file\n",
    "        # keep entries if their nean probabilities (column 10) are greater than 0.5\n",
    "        awk -v pop=\"$pop\" '{ if ($10+0 > 0.5) print $2 \"\\t\" $4 \"\\t\" $10 \"\\t\" $11 \"\\t\" pop}' $file >> $OUTPUTDIR/pops.bed\n",
    "    done\n",
    "done\n",
    "\n",
    "# remove extra file lines except ones starting with numbers (chromosomes), X (for chromosome X), and C (header line)\n",
    "grep '^[0-9 | X | C]' $OUTPUTDIR/pops.bed > $OUTPUTDIR/neanderthal_introgressed_variants_pops.bed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.save_bed_only(full_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/neanderthal_introgressed_variants_pops.bed', \n",
    "              output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/neanderthal_introgressed_variants.bg', \n",
    "              prefix=False, \n",
    "              save_with_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with chr prefix\n",
    "tools.save_bed_only(full_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/neanderthal_introgressed_variants_pops.bed', \n",
    "              output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/neanderthal_introgressed_variants_chr_prefix.bg', \n",
    "              prefix=True,\n",
    "              save_with_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make EUR variant files\n",
    "sankararaman_2016_1 = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/neanderthal_introgressed_variants_pops.bed', \n",
    "                            sep='\\t', low_memory=False)\n",
    "\n",
    "sankararaman_2016_1['Ancestry'] = sankararaman_2016_1['Population'].apply(tools.map_status)\n",
    "sankararaman_2016_1_EUR=sankararaman_2016_1[sankararaman_2016_1['Ancestry']=='EUR']\n",
    "\n",
    "tools.save_bed_only(df=sankararaman_2016_1_EUR, \n",
    "         output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/neanderthal_introgressed_variants_EUR_chr_prefix.bed', \n",
    "         prefix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sankararaman 2016 - 2\n",
    "%%bash\n",
    "\n",
    "# set up directories\n",
    "INPUTDIR=\"/wynton/home/capra/ychen39/introgression_methods/data/sankararaman_2016/summaries/2/neandertal\"\n",
    "OUTPUTDIR=\"/wynton/home/capra/ychen39/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2\"\n",
    "\n",
    "# add column names\n",
    "echo -e \"Chromosome\\tEnd\\tAlleleScore\\tHaplotypeScore\\tPopulation\" > $OUTPUTDIR/pops.bed\n",
    "# add data from each population file\n",
    "for pop in $(cat $INPUTDIR/../../pops); do\n",
    "    cd $INPUTDIR/\"$pop\"/summaries/\n",
    "    FILES=`ls --color=never pred*`\n",
    "    for file in $FILES; do\n",
    "        echo $file\n",
    "        # keep entries if their nean probabilities from derived alleles (column 10) are greater than 0.5\n",
    "        awk -v pop=\"$pop\" '{print $1 print $2 \"\\t\" $4 \"\\t\" $10 \"\\t\" $11 \"\\t\" pop}' $file >> $OUTPUTDIR/pops.bed\n",
    "    done\n",
    "done\n",
    "\n",
    "# remove extra file lines except ones starting with numbers (chromosomes), X (for chromosome X), and C (header line)\n",
    "grep '^[0-9 | X | C]' $OUTPUTDIR/pops.bed > $OUTPUTDIR/neanderthal_introgressed_variants_pops\n",
    "\n",
    "# sort -k1,1 -k2,2n {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/neanderthal_introgressed_variants.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/neanderthal_introgressed_variants_sorted.bg\n",
    "# sort -k1,1 -k2,2n {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/original_deserts.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/original_deserts_sorted.bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.save_bed_only(full_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/neanderthal_introgressed_variants_pops.bed', \n",
    "              output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/neanderthal_introgressed_variants.bg', \n",
    "              prefix=False, \n",
    "              save_with_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with chr prefix\n",
    "tools.save_bed_only(full_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/neanderthal_introgressed_variants_pops.bed', \n",
    "              output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/neanderthal_introgressed_variants_chr_prefix.bg', \n",
    "              prefix=True, \n",
    "              save_with_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make EUR variant files\n",
    "sankararaman_2016_2 = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/neanderthal_introgressed_variants_pops.bed', \n",
    "                            sep='\\t', low_memory=False)\n",
    "\n",
    "sankararaman_2016_2['Ancestry'] = sankararaman_2016_2['Population'].apply(tools.map_status)\n",
    "sankararaman_2016_2_EUR=sankararaman_2016_2[sankararaman_2016_2['Ancestry']=='EUR']\n",
    "\n",
    "tools.save_bed_only(df=sankararaman_2016_2_EUR, \n",
    "         output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/neanderthal_introgressed_variants_EUR_chr_prefix.bed', prefix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Browning 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/browning_2018’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/browning_2018/neanderthal_introgressed_variants_pops.bed\n"
     ]
    }
   ],
   "source": [
    "tools.browning_2018_raw_to_bed(input_dir=f'{HOMEDIR}/introgression_methods/data/browning_2018',\n",
    "                               output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018', \n",
    "                               output_file_name='neanderthal_introgressed_variants_pops.bed',\n",
    "                               archaic='Neanderthal',\n",
    "                               snp_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.save_bed_only(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/neanderthal_introgressed_variants_pops.bed', \n",
    "        output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/neanderthal_introgressed_variants_chr_prefix.bg', prefix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with chr prefix\n",
    "tools.save_bed_only(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/neanderthal_introgressed_variants_pops.bed', \n",
    "              output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/neanderthal_introgressed_variants_chr_prefix.bg', \n",
    "              prefix=True,\n",
    "              save_with_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make EUR variant files\n",
    "browning_2018 = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/neanderthal_introgressed_variants_pops.bed', \n",
    "                            sep='\\t')\n",
    "\n",
    "browning_2018_EUR=browning_2018[browning_2018['Ancestry']=='EUR']\n",
    "\n",
    "tools.save_bed_only(df=browning_2018_EUR, \n",
    "         output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/neanderthal_introgressed_variants_EUR_chr_prefix.bed', prefix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Skov 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading liftover chains\n",
      "Mapping coordinates\n"
     ]
    }
   ],
   "source": [
    "tools.skov_2020_raw_to_bed(input_file=f'{HOMEDIR}/introgression_methods/data/skov_2020/41586_2020_2225_MOESM4_ESM.txt',\n",
    "                                output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020', \n",
    "                                output_file_name='neanderthal_introgressed_variants_pops.bed',\n",
    "                                archaic='Neanderthal',\n",
    "                                snp_level=True,\n",
    "                                liftover_path='/wynton/group/capra/bin/liftOver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.save_bed_only(full_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/neanderthal_introgressed_variants_pops.bed', \n",
    "        output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/neanderthal_introgressed_variants.bg', prefix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with chr prefix\n",
    "tools.save_bed_only(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/neanderthal_introgressed_variants_pops.bed', \n",
    "              output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/neanderthal_introgressed_variants_chr_prefix.bg', \n",
    "              prefix=True,\n",
    "              save_with_header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Vernot 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# set up directories\n",
    "HOMEDIR=/wynton/home/capra/ychen39/\n",
    "INPUTDIR=${HOMEDIR}/introgression_methods/data/vernot_2016/introgressed_tag_snp_frequencies\n",
    "OUTPUTDIR=${HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/\n",
    "\n",
    "# for SNPs files (not extended LD)\n",
    "# add data from each population file\n",
    "for i in ASN EUR PNG SAS; do\n",
    "    awk -v pop=\"$i\" '{print $1 \"\\t\" $2 \"\\t\" $3 \"\\t\" $4 \"\\t\" $5 \"\\t\" $14 \"\\t\" $15 \"\\t\" $16 \"\\t\" pop}' \\\n",
    "    $INPUTDIR/all_tag_snps.\"$i\".merged.ALL.0.3_R2_cluster.1KG_phase3_essentials.bed >> $OUTPUTDIR/neanderthal_introgressed_variants_pops.bed # correct number of snps, should be 526,225\n",
    "done\n",
    "# sort by chromosome\n",
    "sort -u $OUTPUTDIR/neanderthal_introgressed_variants_pops.bed | sort -k 1,1 -k2,2n > $OUTPUTDIR/tmp; mv $OUTPUTDIR/tmp $OUTPUTDIR/neanderthal_introgressed_variants_pops.bed\n",
    "# add column names\n",
    "sed -i '1s/^/Chromosome\\tStart\\tStop\\tAncestralAllele\\tDerivedAllele\\tNeanderthalBase\\tDenisovaBase\\tHaplotypeTag\\tAncestry\\n/' $OUTPUTDIR/neanderthal_introgressed_variants_pops.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.save_bed_only(full_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_pops.bed', \n",
    "        output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants.bg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with chr prefix\n",
    "tools.save_bed_only(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_pops.bed', \n",
    "              output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_chr_prefix.bg', \n",
    "              prefix=True,\n",
    "              save_with_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# set up directories\n",
    "HOMEDIR=/wynton/home/capra/ychen39/\n",
    "INPUTDIR=${HOMEDIR}/introgression_methods/data/vernot_2016/introgressed_tag_snp_frequencies\n",
    "OUTPUTDIR=${HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/\n",
    "\n",
    "# for extended LD files\n",
    "# add data from each population file\n",
    "for i in ASN EUR PNG SAS; do\n",
    "    awk -v pop=\"$i\" '{print $1 \"\\t\" $2 \"\\t\" $3 \"\\t\" $4 \"\\t\" pop}' \\\n",
    "    $INPUTDIR/all_tag_snps.\"$i\".merged.ALL.0.3_R2_cluster.1KG_phase3_essentials.bed.extended_LD.sorted >> $OUTPUTDIR/neanderthal_introgressed_variants_pops_extendedLD.bed\n",
    "done\n",
    "# sort by chromosome\n",
    "sort -u $OUTPUTDIR/neanderthal_introgressed_variants_pops_extendedLD.bed | sort -k 1,1 -k2,2n > $OUTPUTDIR/tmp; mv $OUTPUTDIR/tmp $OUTPUTDIR/neanderthal_introgressed_variants_pops_extendedLD.bed\n",
    "# add column names\n",
    "sed -i '1s/^/Chromosome\\tStart\\tStop\\tHaplotypeTag\\tAncestry\\n/' $OUTPUTDIR/neanderthal_introgressed_variants_pops_extendedLD.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.save_bed_only(full_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_pops_extendedLD.bed', \n",
    "        output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_extendedLD.bg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with chr prefix\n",
    "tools.save_bed_only(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_pops_extendedLD.bed', \n",
    "              output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_extendedLD_chr_prefix.bg', \n",
    "              prefix=True,\n",
    "              save_with_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# set up directories\n",
    "HOMEDIR=/wynton/home/capra/ychen39/\n",
    "INPUTDIR=${HOMEDIR}/introgression_methods/data/vernot_2016/introgressed_tag_snp_frequencies\n",
    "OUTPUTDIR=${HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/\n",
    "\n",
    "# for median SNPs + extended LD\n",
    "# add data from each population file\n",
    "for i in ASN EUR PNG SAS; do\n",
    "    awk -v pop=\"$i\" '{print $1 \"\\t\" $2 \"\\t\" $3 \"\\t\" $11 \"\\t\" $12 \"\\t\" pop}' \\\n",
    "    $INPUTDIR/all_tag_snps.\"$i\".merged.ALL.0.3_R2_cluster.1KG_phase3_essentials.median_af.bed.extended_LD >> $OUTPUTDIR/neanderthal_introgressed_variants_pops_medianextendedLD.bed\n",
    "done\n",
    "# sort by chromosome\n",
    "sort -u $OUTPUTDIR/neanderthal_introgressed_variants_pops_medianextendedLD.bed | sort -k 1,1 -k2,2n > $OUTPUTDIR/tmp; mv $OUTPUTDIR/tmp $OUTPUTDIR/neanderthal_introgressed_variants_pops_medianextendedLD.bed\n",
    "# add column names\n",
    "sed -i '1s/^/Chromosome\\tStart\\tStop\\tSNPsOnHaplotype\\tLength\\tAncestry\\n/' $OUTPUTDIR/neanderthal_introgressed_variants_pops_medianextendedLD.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.save_bed_only(full_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_pops_medianextendedLD.bed', \n",
    "        output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_medianextendedLD.bg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with chr prefix\n",
    "tools.save_bed_only(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_pops_medianextendedLD.bed', \n",
    "              output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_medianextendedLD_chr_prefix.bg', \n",
    "              prefix=True,\n",
    "              save_with_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make EUR variant files\n",
    "vernot_2016 = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_pops.bed', \n",
    "                            sep='\\t')\n",
    "\n",
    "vernot_2016_EUR=vernot_2016[vernot_2016['Ancestry']=='EUR']\n",
    "\n",
    "tools.save_bed_only(df=vernot_2016_EUR, \n",
    "         output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_EUR_chr_prefix.bed', prefix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generate variant overlap file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column of 1s for a \"score\" to make proper bedgraphs\n",
    "\n",
    "!sed 's/$/\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/neanderthal_introgressed_variants.bg > \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/neanderthal_introgressed_variants_score.bg\n",
    "!sed 's/$/\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants.bg > \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_score.bg\n",
    "!sed 's/$/\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_medianextendedLD.bg > \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_medianextendedLD_score.bg\n",
    "!sed 's/$/\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_extendedLD.bg > \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_extendedLD_score.bg\n",
    "!sed 's/$/\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/neanderthal_introgressed_variants.bg > \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/neanderthal_introgressed_variants_score.bg\n",
    "!sed 's/$/\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/neanderthal_introgressed_variants.bg > \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/neanderthal_introgressed_variants_score.bg\n",
    "!sed 's/$/\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/neanderthal_introgressed_variants.bg > \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/neanderthal_introgressed_variants_score.bg\n",
    "!sed 's/$/\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/neanderthal_introgressed_variants.bg > \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/neanderthal_introgressed_variants_score.bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bedtools requires '#' to be at the front of each header, but not using a header here\n",
    "# create union file\n",
    "!$BEDTOOLSDIR/bedtools unionbedg -i {HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/neanderthal_introgressed_variants_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_medianextendedLD_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_variants_extendedLD_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/neanderthal_introgressed_variants_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/neanderthal_introgressed_variants_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/neanderthal_introgressed_variants_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/neanderthal_introgressed_variants_score.bg \\\n",
    "    -header -names sprime vernot_2016 vernot_2016_extendedLD vernot_2016_medianextendedLD sankararaman_2014 sankararaman_2016_1 sankararaman_2016_2 skov_2020  > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_variants.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sprime</th>\n",
       "      <th>vernot_2016</th>\n",
       "      <th>vernot_2016_extendedLD</th>\n",
       "      <th>vernot_2016_medianextendedLD</th>\n",
       "      <th>sankararaman_2014</th>\n",
       "      <th>sankararaman_2016_1</th>\n",
       "      <th>sankararaman_2016_2</th>\n",
       "      <th>skov_2020</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617413</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617414</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617415</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617416</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617417</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1617418 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sprime  vernot_2016  vernot_2016_extendedLD  \\\n",
       "0          True        False                   False   \n",
       "1          True        False                   False   \n",
       "2          True        False                   False   \n",
       "3          True        False                   False   \n",
       "4          True        False                   False   \n",
       "...         ...          ...                     ...   \n",
       "1617413   False        False                   False   \n",
       "1617414   False        False                   False   \n",
       "1617415   False        False                   False   \n",
       "1617416   False        False                   False   \n",
       "1617417   False        False                   False   \n",
       "\n",
       "         vernot_2016_medianextendedLD  sankararaman_2014  sankararaman_2016_1  \\\n",
       "0                               False              False                False   \n",
       "1                               False              False                False   \n",
       "2                               False              False                False   \n",
       "3                               False              False                False   \n",
       "4                               False              False                False   \n",
       "...                               ...                ...                  ...   \n",
       "1617413                         False              False                 True   \n",
       "1617414                         False              False                 True   \n",
       "1617415                         False              False                 True   \n",
       "1617416                         False              False                 True   \n",
       "1617417                         False              False                 True   \n",
       "\n",
       "         sankararaman_2016_2  skov_2020  length  \n",
       "0                      False      False       1  \n",
       "1                      False      False       1  \n",
       "2                      False      False       1  \n",
       "3                      False      False       1  \n",
       "4                      False      False       1  \n",
       "...                      ...        ...     ...  \n",
       "1617413                False      False       1  \n",
       "1617414                False      False       1  \n",
       "1617415                False      False       1  \n",
       "1617416                False      False       1  \n",
       "1617417                False      False       1  \n",
       "\n",
       "[1617418 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_variants.bed', sep='\\t', low_memory=False)\n",
    "# replace 0s with NA\n",
    "overlap.replace(0, np.nan, inplace=True)\n",
    "# add length data\n",
    "overlap['length'] = overlap['end'] - overlap['start']\n",
    "overlap\n",
    "\n",
    "overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_variants.bed', sep='\\t', index=False)\n",
    "\n",
    "# save bed file version of all identified introgressed segments across methods\n",
    "overlap[['chrom', 'start', 'end']].to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/overlap_bed_only_variants.bed', sep='\\t', index=False)\n",
    "\n",
    "# first, convert dataframe to boolean values\n",
    "# exclude for boolean\n",
    "exclude_columns = ['chrom', 'start', 'end', 'length']\n",
    "# convert to boolean (exclude columns specified)\n",
    "boolean_overlap = overlap.copy()\n",
    "boolean_overlap[boolean_overlap.columns.difference(exclude_columns)] = boolean_overlap[boolean_overlap.columns.difference(exclude_columns)].applymap(lambda x: pd.notna(x))\n",
    "boolean_overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_overlap_variants.bed', sep='\\t', index=False)\n",
    "boolean_overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_overlap_no_header_variants.bed', sep='\\t', index=False, header=False)\n",
    "\n",
    "# drop chrom, start, end\n",
    "boolean_overlap = boolean_overlap.drop(['chrom', 'start', 'end'], axis=1)\n",
    "boolean_overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_overlap_no_bed_variants', sep='\\t', index=False)\n",
    "boolean_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating standardized fragment files across introgression methods\n",
    "\n",
    "Here, we use the introgression_tools package to generate standardized individual-level and superpopulation-level introgression data.\n",
    "\n",
    "Additionally, we will generate \"2 Percent\" files for individual-level data: we will take the top introgressed regions for each person, consisting of 2% of each individual' genome, so we can repeat individual-level analysis subsetted to \"confident\" regions and controlling for the same amount of introgression across people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will convert all raw outputs to standardized .bed and .bg files for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IBDMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/chen_2020/’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/chen_2020//bedgraphs’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/chen_2020/’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/chen_2020//bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/chen_2020//superpop_nean_chen2020_frag.bed\n"
     ]
    }
   ],
   "source": [
    "tools.chen_2020_raw_to_bed(input_path=f'{HOMEDIR}/introgression_methods/data/ibdmix_chen_et_al_2020/Neanderthal sequence in 1000 genome.50kb.txt', \n",
    "                           output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/', \n",
    "                           output_file_name='individual_nean_chen2020_frag.bed', \n",
    "                           individual=True, population=False)\n",
    "\n",
    "tools.chen_2020_raw_to_bed(input_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/individual_nean_chen2020_frag.bed', \n",
    "                           output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/', \n",
    "                           output_file_name='superpop_nean_chen2020_frag.bed', \n",
    "                           population=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/chen_2020/’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/chen_2020//bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAS\n",
      "merged\n",
      "EUR\n",
      "merged\n",
      "AMR\n",
      "merged\n",
      "SAS\n",
      "merged\n",
      "AFR\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/chen_2020//superpop_nean_chen2020_frag.bed\n"
     ]
    }
   ],
   "source": [
    "tools.chen_2020_raw_to_bed(input_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/individual_nean_chen2020_frag.bed', \n",
    "                           output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/', \n",
    "                           output_file_name='superpop_nean_chen2020_frag.bed', \n",
    "                           population=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also save separate dataframe with African admixed individuals only\n",
    "ibdmix = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/ibdmix_chen2020/individual_introgressed_chen2020_frag.bed',\n",
    "                     sep='\\t')\n",
    "ibdmix_afr_admix = ibdmix[ibdmix.Population.isin(['ACB', 'ASW'])][['Chromosome', 'Start', 'End', 'Score']]\n",
    "# print the AFR ids\n",
    "print('IBDMix AFR (including admixed) individuals:', len(ibdmix[ibdmix.Population.isin(['ACB', 'ASW'])].ID.unique()))\n",
    "ibdmix_afr_admix.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr_admix.bed', sep='\\t', index=False, header=False)\n",
    "ibdmix_afr = ibdmix[ibdmix.Population.isin(['LWK', 'GWD', 'MSL', 'YRI', 'ESN'])][['Chromosome', 'Start', 'End', 'Score']]\n",
    "\n",
    "# print the AFR ids\n",
    "print('IBDMix AFR (no admixed) individuals:', len(ibdmix[ibdmix.Population.isin(['LWK', 'GWD', 'MSL', 'YRI', 'ESN'])].ID.unique()))\n",
    "\n",
    "ibdmix_afr.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr.bed', sep='\\t', index=False, header=False)\n",
    "\n",
    "\n",
    "# create merged file\n",
    "!sort -k1,1 -k2,2n {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr_admix.bed > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr_admix_sorted.bed\n",
    "!$BEDTOOLSDIR/bedtools merge -i {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr_admix_sorted.bed > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr_admix_merged_sorted.bed\n",
    "!rm {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr_admix.bed {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr_admix_sorted.bed\n",
    "\n",
    "!sort -k1,1 -k2,2n {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr.bed > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr_sorted.bed\n",
    "!$BEDTOOLSDIR/bedtools merge -i {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr_sorted.bed > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr_merged_sorted.bed\n",
    "!rm {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr.bed {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/afr_sorted.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBDMix AFR (no admixed) individuals: 504\n",
      "IBDMix AFR (including admixed) individuals: 661\n"
     ]
    }
   ],
   "source": [
    "print('IBDMix AFR (no admixed) individuals:', len(ibdmix[ibdmix.Population.isin(['LWK', 'GWD', 'MSL', 'YRI', 'ESN'])].ID.unique()))\n",
    "print('IBDMix AFR (including admixed) individuals:', len(ibdmix[ibdmix.Population.isin(['ACB', 'ASW'])].ID.unique()) + len(ibdmix[ibdmix.Population.isin(['LWK', 'GWD', 'MSL', 'YRI', 'ESN'])].ID.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ArchaicSeeker2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/yuan_2021’: File exists\n",
      "/wynton/home/capra/ychen39/.conda/envs/jupyter/lib/python3.11/site-packages/pyranges/methods/init.py:45: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  return {k: v for k, v in df.groupby(grpby_key)}\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/yuan_2021//bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAS\n",
      "merged\n",
      "SAS\n",
      "merged\n",
      "EUR\n",
      "merged\n",
      "Oceania\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/yuan_2021//superpop_nean_yuan2021_frag.bed\n"
     ]
    }
   ],
   "source": [
    "tools.yuan_2021_raw_to_bed(input_dir_list=[f'{HOMEDIR}/introgression_methods/data/archaicseeker2.0_yuan_et_al_2021/IntrogressedSeg/KGP/EastAsia', \n",
    "                                       f'{HOMEDIR}/introgression_methods/data/archaicseeker2.0_yuan_et_al_2021/IntrogressedSeg/KGP/SouthAsia', \n",
    "                                       f'{HOMEDIR}/introgression_methods/data/archaicseeker2.0_yuan_et_al_2021/IntrogressedSeg/KGP/WestEurasia',\n",
    "                                       f'{HOMEDIR}/introgression_methods/data/archaicseeker2.0_yuan_et_al_2021/IntrogressedSeg/SGDP'],\n",
    "                           output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021', \n",
    "                           output_file_name='individual_nean_yuan2021_frag.bed', \n",
    "                           archaic='Neanderthal',\n",
    "                           individual=True)\n",
    "\n",
    "tools.yuan_2021_raw_to_bed(file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/individual_nean_yuan2021_frag.bed',\n",
    "                           output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/', \n",
    "                           output_file_name='superpop_nean_yuan2021_frag.bed', \n",
    "                           archaic='Neanderthal',\n",
    "                           population=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/yuan_2021/debugging/’: File exists\n",
      "/wynton/home/capra/ychen39/.conda/envs/jupyter/lib/python3.11/site-packages/pyranges/methods/init.py:45: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  return {k: v for k, v in df.groupby(grpby_key)}\n"
     ]
    }
   ],
   "source": [
    "## retry, removed \"contains\" to exact match for \"Neanderthal\" and \"Denisovan\"\n",
    "\n",
    "tools.yuan_2021_raw_to_bed(input_dir_list=[f'{HOMEDIR}/introgression_methods/data/archaicseeker2.0_yuan_et_al_2021/IntrogressedSeg/KGP/EastAsia', \n",
    "                                       f'{HOMEDIR}/introgression_methods/data/archaicseeker2.0_yuan_et_al_2021/IntrogressedSeg/KGP/SouthAsia', \n",
    "                                       f'{HOMEDIR}/introgression_methods/data/archaicseeker2.0_yuan_et_al_2021/IntrogressedSeg/KGP/WestEurasia',\n",
    "                                       f'{HOMEDIR}/introgression_methods/data/archaicseeker2.0_yuan_et_al_2021/IntrogressedSeg/SGDP'],\n",
    "                           output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/debugging/', \n",
    "                           output_file_name='individual_nean_yuan2021_frag.bed', \n",
    "                           archaic='Neanderthal',\n",
    "                           individual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ArchIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/durvasula_2019’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CEU.chr22-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr6-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr15-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr8-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr10-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr12-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr21-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr19-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr9-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr17-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr18-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr7-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr2-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr3-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr20-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr4-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr16-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr1-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr14-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr5-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr11-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr13-prediction.bed.gz-trimmed.bed.gz\n"
     ]
    }
   ],
   "source": [
    "# using an increased threshold so we get ~2% introgression per person\n",
    "tools.durvasula_2019_raw_to_bed(input_dir=f'{HOMEDIR}/introgression_methods/data/archie_durvasula2019/CEU-predictions',\n",
    "                                ancestry='EUR',\n",
    "                                output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019', \n",
    "                                output_file_name='individual_nean_durvasula2019_frag_higher_threshold.bed', \n",
    "                                individual=True,\n",
    "                                threshold=0.9984,\n",
    "                                keep_old_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/durvasula_2019’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CEU.chr22-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr6-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr15-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr8-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr10-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr12-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr21-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr19-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr9-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr17-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr18-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr7-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr2-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr3-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr20-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr4-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr16-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr1-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr14-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr5-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr11-prediction.bed.gz-trimmed.bed.gz\n",
      "Reading CEU.chr13-prediction.bed.gz-trimmed.bed.gz\n"
     ]
    }
   ],
   "source": [
    "# using the threshold reported in the paper\n",
    "tools.durvasula_2019_raw_to_bed(input_dir=f'{HOMEDIR}/introgression_methods/data/archie_durvasula2019/CEU-predictions',\n",
    "                                ancestry='EUR',\n",
    "                                output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019', \n",
    "                                output_file_name='individual_nean_durvasula2019_frag_thresholded_0.862.bed', \n",
    "                                individual=True,\n",
    "                                threshold=0.862,\n",
    "                                keep_old_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/durvasula_2019’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/durvasula_2019/bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUR\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/durvasula_2019/superpop_nean_durvasula2019_frag.bed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: target 'union_merged.bg' is not a directory\n"
     ]
    }
   ],
   "source": [
    "tools.durvasula_2019_raw_to_bed(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019',\n",
    "                                ancestry='EUR',\n",
    "                                file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019/individual_nean_durvasula2019_frag_higher_threshold.bed',\n",
    "                                output_file_name='superpop_nean_durvasula2019_frag.bed', \n",
    "                                population=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/durvasula_2019’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/durvasula_2019/bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUR\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/durvasula_2019/superpop_nean_durvasula2019_frag_thresholded_0.862.bed\n"
     ]
    }
   ],
   "source": [
    "tools.durvasula_2019_raw_to_bed(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019',\n",
    "                                ancestry='EUR',\n",
    "                                file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019/individual_nean_durvasula2019_frag.bed',\n",
    "                                output_file_name='superpop_nean_durvasula2019_frag_thresholded_0.862.bed', \n",
    "                                population=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DICAL-ADMIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yaenchen/Library/CloudStorage/OneDrive-NortheasternUniversity/UCSF/CapraLab//introgression_methods/data/dical-admix_steinrucken_et_al_2018/called_tracts/CEU_lax_chr1_beds.tar.gz\n",
      "/Users/yaenchen/Library/CloudStorage/OneDrive-NortheasternUniversity/UCSF/CapraLab//introgression_methods/data/dical-admix_steinrucken_et_al_2018/called_tracts/CEU_lax_chr1_beds.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: /Users/yaenchen/Library/CloudStorage/OneDrive-NortheasternUniversity/UCSF/CapraLab//introgression_methods/introgression_tools/test/dicaladmix/: File exists\n"
     ]
    }
   ],
   "source": [
    "#tools.steinruecken_2018_raw_to_bed(input_dir=f'{HOMEDIR}/introgression_methods/data/steinrucken_2018/called_tracts',\n",
    "                                output_dir=f'{HOMEDIR}/introgression_methods/introgression_tools/test/dicaladmix/', \n",
    "                                output_file_name='individual_nean_introgressed_steinrucken2018_frag.bed',\n",
    "                                prob_dir= f'{HOMEDIR}/introgression_methods/data/dical-steinrucken_2018/posterior_probabilities',\n",
    "                                input_tar_gz='CEU_lax_chr1_beds.tar.gz',\n",
    "                                individual=True) \n",
    "\n",
    "## did not run this, used bash script instead. \n",
    "## already generated individual=level file with posterior probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/steinruecken_2018/’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/steinruecken_2018/bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEU\n",
      "merged\n",
      "CHBS\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/steinruecken_2018/superpop_nean_steinruecken2018_frag.bed\n"
     ]
    }
   ],
   "source": [
    "tools.steinruecken_2018_raw_to_bed(file_path=f'{HOMEDIR}/introgression_methods/cleaned/steinruecken_2018/individual_introgressed_steinruecken2018_frag.bed',\n",
    "                                output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/steinruecken_2018', \n",
    "                                output_file_name='superpop_nean_steinruecken2018_frag.bed',\n",
    "                                population=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Skov2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/skov_2020/’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/skov_2020/bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Icelandic\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/skov_2020/superpop_nean_skov2020_frag.bed\n"
     ]
    }
   ],
   "source": [
    "tools.skov_2020_raw_to_bed(input_file=f'{HOMEDIR}/introgression_methods/data/skov_2020/hg38_to_hg19_liftover/skov_2020_lifted.bed',\n",
    "                                output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020', \n",
    "                                output_file_name='superpop_nean_skov2020_frag.bed',\n",
    "                                population=True,\n",
    "                                archaic='Neanderthal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also clean variants file\n",
    "#awk '{print $1,$2,$2+1}' {HOMEDIR}/introgression_methods/data/skov_2020/41586_2020_2225_MOESM4_ESM.txt > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/snps.bed\n",
    "\n",
    "skov_2020_variants = pd.read_csv(f'{HOMEDIR}/introgression_methods/data/skov_2020/41586_2020_2225_MOESM4_ESM.txt', sep='\\t', low_memory=False)\n",
    "# add end position\n",
    "skov_2020_variants['start'] = skov_2020_variants['pos'] - 1\n",
    "#print(skov_2020_variants.snptype.unique())\n",
    "# keep only DAV snps\n",
    "skov_2020_variants = skov_2020_variants[(skov_2020_variants['snptype'] == 'DAVsnp') | (skov_2020_variants['snptype'] == 'linkedDAVsnp')]\n",
    "# rename pos to end\n",
    "skov_2020_variants.rename(columns={'pos': 'end'}, inplace=True)\n",
    "# keep only bed positions\n",
    "skov_2020_variants_bed = skov_2020_variants[['chrom', 'start', 'end']]\n",
    "skov_2020_variants_bed\n",
    "# save to file -- version with chr prefix for liftover\n",
    "skov_2020_variants_bed.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/dav_and_linked_snps_hg38_chr_prefix.bg', header=False, sep='\\t', index=False)\n",
    "#drop 'chr' prefix\n",
    "skov_2020_variants_bed['chrom'] = skov_2020_variants_bed['chrom'].str.replace('chr', '')\n",
    "# save to file\n",
    "skov_2020_variants_bed.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/dav_and_linked_snps_hg38.bg', header=False, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SARGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarge = pd.read_csv(f'{HOMEDIR}/introgression_methods/data/sarge_schaefer2021/sarge_admixed_haps_hs37d5.bed', sep='\\t')\n",
    "len(sarge['hap'][sarge['pop'] == 'Africa'].unique()) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/schaefer_2021/’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/schaefer_2021//bedgraphs’: File exists\n"
     ]
    }
   ],
   "source": [
    "tools.schaefer_2021_raw_to_bed(input_file=f'{HOMEDIR}/introgression_methods/data/sarge_schaefer2021/sarge_admixed_haps_hs37d5.bed',\n",
    "                                output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/schaefer_2021/', \n",
    "                                output_file_name='individual_nean_schaefer2021_frag.bed',\n",
    "                                individual=True,\n",
    "                                archaic='Neanderthal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/schaefer_2021/’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/schaefer_2021//bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oceania\n",
      "merged\n",
      "WestEurasia\n",
      "merged\n",
      "EastAsia\n",
      "merged\n",
      "Africa\n",
      "merged\n",
      "Africa2\n",
      "merged\n",
      "America\n",
      "merged\n",
      "CentralAsiaSiberia\n",
      "merged\n",
      "SouthAsia\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/schaefer_2021//superpop_nean_schaefer2021_frag.bed\n"
     ]
    }
   ],
   "source": [
    "tools.schaefer_2021_raw_to_bed(input_file=f'{HOMEDIR}/introgression_methods/data/sarge_schaefer2021/sarge_admixed_haps_hs37d5.bed',\n",
    "                                output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/schaefer_2021/', \n",
    "                                output_file_name='superpop_nean_schaefer2021_frag.bed',\n",
    "                                population=True,\n",
    "                                archaic='Neanderthal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ARGWeaver-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020/mandenka_individual_nean_hubisz2020_frag.bed\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020/papuan_individual_nean_hubisz2020_frag.bed\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020/khomani_san_individual_nean_hubisz2020_frag.bed\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020/basque_individual_nean_hubisz2020_frag.bed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020/bedgraphs’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020/bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020/superpop_nean_hubisz2020_frag.bed\n"
     ]
    }
   ],
   "source": [
    "## Process individual files first\n",
    "tools.hubisz_2020_raw_to_bed(input_path=f'{HOMEDIR}/introgression_methods/data/argweaver-d_hubisz_et_al_2020/ooaM1A/Mandenka_2F.bed', \n",
    "                               ancestry='Africa', \n",
    "                               ID='LP6005441-DNA_F07',\n",
    "                               archaic='Neanderthal',\n",
    "                               output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020',\n",
    "                               output_file_name='mandenka_individual_nean_hubisz2020_frag.bed',\n",
    "                               individual=True)\n",
    "\n",
    "tools.hubisz_2020_raw_to_bed(input_path=f'{HOMEDIR}/introgression_methods/data/argweaver-d_hubisz_et_al_2020/ooaM1A/Papuan_1F.bed', \n",
    "                               ancestry='Oceania', \n",
    "                               ID='LP6005441-DNA_B10',\n",
    "                               archaic='Neanderthal',\n",
    "                               output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020',\n",
    "                               output_file_name='papuan_individual_nean_hubisz2020_frag.bed',\n",
    "                               individual=True)\n",
    "\n",
    "tools.hubisz_2020_raw_to_bed(input_path=f'{HOMEDIR}/introgression_methods/data/argweaver-d_hubisz_et_al_2020/ooaM1A/Khomani_San_1F.bed', \n",
    "                               ancestry='Africa', \n",
    "                               ID='LP6005677-DNA_D03',\n",
    "                               archaic='Neanderthal',\n",
    "                               output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020',\n",
    "                               output_file_name='khomani_san_individual_nean_hubisz2020_frag.bed',\n",
    "                               individual=True)\n",
    "\n",
    "tools.hubisz_2020_raw_to_bed(input_path=f'{HOMEDIR}/introgression_methods/data/argweaver-d_hubisz_et_al_2020/ooaM1A/Basque_2F.bed', \n",
    "                               ancestry='WestEurasia', \n",
    "                               ID='LP6005441-DNA_D02',\n",
    "                               archaic='Neanderthal',\n",
    "                               output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020',\n",
    "                               output_file_name='basque_individual_nean_hubisz2020_frag.bed',\n",
    "                               individual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020/bedgraphs’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020/bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Africa\n",
      "merged\n",
      "Oceania\n",
      "merged\n",
      "WestEurasia\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/hubisz_2020/superpop_nean_hubisz2020_frag.bed\n"
     ]
    }
   ],
   "source": [
    "## Process population files, concatenate\n",
    "tools.hubisz_2020_raw_to_bed(individual_files=[f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020/mandenka_individual_nean_hubisz2020_frag.bed',\n",
    "                                               f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020/papuan_individual_nean_hubisz2020_frag.bed',\n",
    "                                               f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020/khomani_san_individual_nean_hubisz2020_frag.bed',\n",
    "                                               f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020/basque_individual_nean_hubisz2020_frag.bed'], \n",
    "                               output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020',\n",
    "                               output_file_name='superpop_nean_hubisz2020_frag.bed',\n",
    "                               population=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/browning_2018’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/browning_2018/bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1\n",
      "chr2\n",
      "chr3\n",
      "chr4\n",
      "chr5\n",
      "chr6\n",
      "chr7\n",
      "chr8\n",
      "chr9\n",
      "chr10\n",
      "chr11\n",
      "chr12\n",
      "chr13\n",
      "chr14\n",
      "chr15\n",
      "chr16\n",
      "chr17\n",
      "chr18\n",
      "chr19\n",
      "chr20\n",
      "chr21\n",
      "chr22\n",
      "EAS\n",
      "merged\n",
      "SAS\n",
      "merged\n",
      "EUR\n",
      "merged\n",
      "AMR\n",
      "merged\n",
      "Oceania\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/browning_2018/superpop_nean_browning2018_frag.bed\n"
     ]
    }
   ],
   "source": [
    "tools.browning_2018_raw_to_bed(input_dir=f'{HOMEDIR}/introgression_methods/data/browning_2018',\n",
    "                               output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018', \n",
    "                               output_file_name='superpop_nean_browning2018_frag.bed',\n",
    "                               archaic='Neanderthal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### S*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# set up directories\n",
    "HOMEDIR=/wynton/home/capra/ychen39/\n",
    "INPUTDIR=${HOMEDIR}/introgression_methods/data/vernot_2016/introgressed_haplotypes\n",
    "OUTPUTDIR=${HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/vernot_2016_round2\n",
    "\n",
    "# for neanderthal\n",
    "# add data from each population file\n",
    "for i in EAS EUR PNG SAS; do\n",
    "    awk -v pop=\"$i\" '{print $1 \"\\t\" $2 \"\\t\" $3 \"\\t\" $5 \"\\t\" pop}' \\\n",
    "    $INPUTDIR/LL.callset\"$i\".mr_0.99.neand_calls_by_hap.bed.merged.by_chr.bed >> $OUTPUTDIR/neanderthal_introgressed_haplotypes_individual.bed\n",
    "done\n",
    "\n",
    "# for denisovan\n",
    "# add data from each population file\n",
    "for i in PNG; do\n",
    "    awk -v pop=\"$i\" '{print $1 \"\\t\" $2 \"\\t\" $3 \"\\t\" $5 \"\\t\" pop}' \\\n",
    "    $INPUTDIR/LL.callset\"$i\".mr_0.99.den_calls_by_hap.bed.merged.by_chr.bed >> $OUTPUTDIR/denisovan_introgressed_haplotypes_individual.bed # correct number of snps, should be 526,225\n",
    "done\n",
    "\n",
    "# sort by chromosome\n",
    "sort -u $OUTPUTDIR/neanderthal_introgressed_haplotypes_individual.bed | sort -k 1,1 -k2,2n > $OUTPUTDIR/tmp; mv $OUTPUTDIR/tmp $OUTPUTDIR/neanderthal_introgressed_haplotypes_individual.bed\n",
    "sort -u $OUTPUTDIR/denisovan_introgressed_haplotypes_individual.bed | sort -k 1,1 -k2,2n > $OUTPUTDIR/tmp; mv $OUTPUTDIR/tmp $OUTPUTDIR/denisovan_introgressed_haplotypes_individual.bed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/vernot_2016//tmp/’: File exists\n"
     ]
    }
   ],
   "source": [
    "tools.vernot_2016_raw_to_bed(raw_introgressed_haplotypes_dir=f'{HOMEDIR}/introgression_methods/data/vernot_2016/introgressed_haplotypes',\n",
    "    introgressed_haplotypes_file=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_haplotypes_individual.bed', \n",
    "                             output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/', \n",
    "                             output_file_name='neanderthal_introgressed_haplotypes_individual.bed',\n",
    "                             archaic='Neanderthal', individual_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/vernot_2016/’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/vernot_2016//bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAS\n",
      "merged\n",
      "SAS\n",
      "merged\n",
      "EUR\n",
      "merged\n",
      "PNG\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/vernot_2016//superpop_nean_vernot2016_frag.bed\n"
     ]
    }
   ],
   "source": [
    "tools.vernot_2016_raw_to_bed(raw_introgressed_haplotypes_dir=f'{HOMEDIR}/introgression_methods/data/vernot_2016/introgressed_haplotypes',\n",
    "                             introgressed_haplotypes_file=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/neanderthal_introgressed_haplotypes_individual.bed', \n",
    "                             output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/', \n",
    "                             output_file_name='superpop_nean_vernot2016_frag.bed',\n",
    "                             archaic='Neanderthal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.vernot_2016_raw_to_bed_snp(snp_input_file=f'{HOMEDIR}/introgression_methods/cleaned/vernot_2016/ancestral_introgressed_variants.bed', \n",
    "                             extended_ld_input_file=f'{HOMEDIR}/introgression_methods/cleaned/vernot_2016/ancestral_introgressed_variants_extendedLD.bed', \n",
    "                             hap_input_file=f'{HOMEDIR}/introgression_methods/cleaned/vernot_2016/ancestral_introgressed_variants_medianextendedLD.bed', \n",
    "                             output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/', \n",
    "                             output_file_name='superpop_nean_vernot2016_frag_round2.bed',\n",
    "                             archaic='Neanderthal') # fixed exact match for derived/archaic alleles, changed to isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/vernot_2016/vernot_2016_round2/denisovan’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/vernot_2016/vernot_2016_round2/denisovan/bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAS\n",
      "merged\n",
      "ASN\n",
      "merged\n",
      "PNG\n",
      "merged\n",
      "EUR\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/vernot_2016/vernot_2016_round2/denisovan/superpop_den_vernot2016_frag_round2.bed\n"
     ]
    }
   ],
   "source": [
    "tools.vernot_2016_raw_to_bed_snp(snp_input_file=f'{HOMEDIR}/introgression_methods/cleaned/vernot_2016/ancestral_introgressed_variants.bed', \n",
    "                             extended_ld_input_file=f'{HOMEDIR}/introgression_methods/cleaned/vernot_2016/ancestral_introgressed_variants_extendedLD.bed', \n",
    "                             hap_input_file=f'{HOMEDIR}/introgression_methods/cleaned/vernot_2016/ancestral_introgressed_variants_medianextendedLD.bed', \n",
    "                             output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/denisovan', \n",
    "                             output_file_name='superpop_den_vernot2016_frag_round2.bed',\n",
    "                             archaic='Denisovan') # fixed exact match for derived/archaic alleles, changed to isin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CRF 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/sankararaman_2014’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/sankararaman_2014/bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUR\n",
      "merged\n",
      "EAS\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/sankararaman_2014/superpop_nean_sankararaman2014_frag.bed\n"
     ]
    }
   ],
   "source": [
    "tools.sankararaman_2014_raw_to_bed(snp_input_file=f'{HOMEDIR}/introgression_methods/cleaned/sankararaman_2014_copy/neanderthal_introgressed_variants_pops.bed', \n",
    "                             hap_input_file=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/1kgids_individual_neanderthal_introgressed_fragments_pops.bed', \n",
    "                             output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014', \n",
    "                             output_file_name='superpop_nean_sankararaman2014_frag.bed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CRF 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/sankararaman_2016_1’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMR\n",
      "merged\n",
      "CentralAsia\n",
      "merged\n",
      "EAS\n",
      "merged\n",
      "Oceania\n",
      "merged\n",
      "SAS\n",
      "merged\n",
      "EUR\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/superpop_nean_sankararaman2016_1_frag.bed\n"
     ]
    }
   ],
   "source": [
    "tools.sankararaman_2016_raw_to_bed(hap_input_file=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/sgdp_individual_neanderthal_introgressed_fragments_pops.bed', \n",
    "                             output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1', \n",
    "                             output_file_name='superpop_nean_sankararaman2016_1_frag.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/sankararaman_2016_2’: File exists\n",
      "mkdir: cannot create directory ‘/wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/bedgraphs’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMR\n",
      "merged\n",
      "CentralAsia\n",
      "merged\n",
      "EAS\n",
      "merged\n",
      "Oceania\n",
      "merged\n",
      "SAS\n",
      "merged\n",
      "EUR\n",
      "merged\n",
      "Output .bed file is located at /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/superpop_nean_sankararaman2016_2_frag.bed\n"
     ]
    }
   ],
   "source": [
    "tools.sankararaman_2016_raw_to_bed(hap_input_file=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/sgdp_individual_neanderthal_introgressed_fragments_pops.bed', \n",
    "                             output_dir=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2', \n",
    "                             output_file_name='superpop_nean_sankararaman2016_2_frag.bed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create union file. This file is generated from the bedgraphs and need to have a # in the first header line, which we did in the previous section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bedtools requires '#' to be at the front of each header\n",
    "\n",
    "!sed '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/bedgraphs/union_merged_highest_score.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/bedgraphs/union_merged_header.bg\n",
    "!sed '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020/bedgraphs/union_merged_highest_score.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020/bedgraphs/union_merged_header.bg\n",
    "!sed '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/bedgraphs/union_merged_highest_score.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/bedgraphs/union_merged_header.bg\n",
    "!sed '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019/bedgraphs/union_merged_highest_score.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019/bedgraphs/union_merged_header.bg\n",
    "!sed '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/bedgraphs/union_merged_highest_score.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/bedgraphs/union_merged_header.bg\n",
    "!sed '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/vernot_2016_round2/bedgraphs/union_merged_highest_score.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/bedgraphs/union_merged_header.bg\n",
    "!sed '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/bedgraphs/union_merged_highest_score.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/bedgraphs/union_merged_header.bg\n",
    "!sed '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/bedgraphs/union_merged_highest_score.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/bedgraphs/union_merged_header.bg\n",
    "!sed '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/bedgraphs/union_merged_highest_score.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/bedgraphs/union_merged_header.bg\n",
    "!sed '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/bedgraphs/union_merged_highest_score.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/bedgraphs/union_merged_header.bg\n",
    "!sed '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/steinruecken_2018/bedgraphs/union_merged_highest_score.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/steinruecken_2018/bedgraphs/union_merged_header.bg\n",
    "!sed '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/schaefer_2021/bedgraphs/union_merged_highest_score.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/schaefer_2021/bedgraphs/union_merged_header.bg\n",
    "\n",
    "# Create union file.\n",
    "!$BEDTOOLSDIR/bedtools unionbedg -i {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/steinruecken_2018/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/schaefer_2021/bedgraphs/union_merged_header.bg \\\n",
    "    -header -names ibdmix argweaverd archaicseeker archie sprime vernot_2016 sankararaman_2014 sankararaman_2016_1 sankararaman_2016_2 skov_2020 steinruecken_2018 sarge > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make overlap file with only bed and chr prefix: overlap_bed_only_no_header_chr_prefix.bg\n",
    "\n",
    "# only keep chrom, start, stop\n",
    "!cut -f1,2,3 {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods.bed > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_bed_only.bed\n",
    "# remove header\n",
    "!sed -e '1d' -i {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_bed_only.bed\n",
    "# add chr prefix\n",
    "!sed -e 's/^/chr/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_bed_only.bed > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/overlap_bed_only_no_header_chr_prefix.bg\n",
    "\n",
    "# create merged file. This file is generated from the bedgraphs and need to have a # in the first header line, which we did in the previous section\n",
    "!$BEDTOOLSDIR/bedtools merge -i {HOMEDIR}/introgression_methods/cleaned/introgression_tools/overlap_bed_only_no_header_chr_prefix.bg \\\n",
    "    > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/merged_all_methods.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ibdmix</th>\n",
       "      <th>argweaverd</th>\n",
       "      <th>archaicseeker</th>\n",
       "      <th>archie</th>\n",
       "      <th>sprime</th>\n",
       "      <th>vernot_2016</th>\n",
       "      <th>sankararaman_2014</th>\n",
       "      <th>sankararaman_2016_1</th>\n",
       "      <th>sankararaman_2016_2</th>\n",
       "      <th>skov_2020</th>\n",
       "      <th>steinruecken_2018</th>\n",
       "      <th>sarge</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>22746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370118</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>109687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370119</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>75368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370120</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>37507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370121</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370122</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370123 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ibdmix  argweaverd  archaicseeker  archie  sprime  vernot_2016  \\\n",
       "0        False       False           True   False   False        False   \n",
       "1        False       False           True   False   False        False   \n",
       "2        False       False           True   False   False        False   \n",
       "3        False       False           True   False   False        False   \n",
       "4        False       False           True   False   False        False   \n",
       "...        ...         ...            ...     ...     ...          ...   \n",
       "370118   False       False          False   False   False        False   \n",
       "370119   False       False          False   False   False        False   \n",
       "370120   False       False          False   False   False        False   \n",
       "370121   False       False          False   False   False        False   \n",
       "370122   False       False          False   False   False        False   \n",
       "\n",
       "        sankararaman_2014  sankararaman_2016_1  sankararaman_2016_2  \\\n",
       "0                   False                False                False   \n",
       "1                   False                False                False   \n",
       "2                   False                False                False   \n",
       "3                   False                False                False   \n",
       "4                   False                False                False   \n",
       "...                   ...                  ...                  ...   \n",
       "370118              False                 True                False   \n",
       "370119              False                 True                False   \n",
       "370120              False                 True                False   \n",
       "370121              False                 True                False   \n",
       "370122              False                False                False   \n",
       "\n",
       "        skov_2020  steinruecken_2018  sarge  length  \n",
       "0           False              False  False    4580  \n",
       "1           False              False  False    5610  \n",
       "2           False              False  False    4454  \n",
       "3           False              False  False   22746  \n",
       "4           False              False  False    9495  \n",
       "...           ...                ...    ...     ...  \n",
       "370118      False              False  False  109687  \n",
       "370119      False              False  False   75368  \n",
       "370120      False              False  False   37507  \n",
       "370121      False              False  False    3066  \n",
       "370122      False               True  False    1500  \n",
       "\n",
       "[370123 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods.bed', sep='\\t', low_memory=False)\n",
    "# replace 0s with NA\n",
    "overlap.replace(0, np.nan, inplace=True)\n",
    "# add length data\n",
    "overlap['length'] = overlap['end'] - overlap['start']\n",
    "overlap\n",
    "\n",
    "overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods.bed', sep='\\t', index=False)\n",
    "\n",
    "# save bed file version of all identified introgressed segments across methods\n",
    "overlap[['chrom', 'start', 'end']].to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/overlap_bed_only.bed', sep='\\t', index=False)\n",
    "\n",
    "# first, convert dataframe to boolean values\n",
    "# exclude for boolean\n",
    "exclude_columns = ['chrom', 'start', 'end', 'length']\n",
    "# convert to boolean\n",
    "boolean_overlap = overlap.copy()\n",
    "boolean_overlap[boolean_overlap.columns.difference(exclude_columns)] = boolean_overlap[boolean_overlap.columns.difference(exclude_columns)].applymap(lambda x: pd.notna(x))\n",
    "boolean_overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_overlap.bed', sep='\\t', index=False)\n",
    "boolean_overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_overlap_no_header.bed', sep='\\t', index=False, header=False)\n",
    "\n",
    "# drop chrom, start, end\n",
    "boolean_overlap = boolean_overlap.drop(['chrom', 'start', 'end'], axis=1)\n",
    "boolean_overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_overlap_no_bed', sep='\\t', index=False)\n",
    "boolean_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "793900000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolean_overlap[boolean_overlap['archie']==True].length.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### no argweaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to open file /wynton/home/capra/ychen39//introgression_methods/cleaned/introgression_tools/chen_2020/bedgraphs/sorted_union_merged_chr_prefix.bg. Exiting.\n"
     ]
    }
   ],
   "source": [
    "## REGION LEVEL FILE\n",
    "# Create union file without ARGWeaver-D (hubisz 2020)\n",
    "!$BEDTOOLSDIR/bedtools unionbedg -i {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/steinruecken_2018/bedgraphs/union_merged_header.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/schaefer_2021/bedgraphs/union_merged_header.bg \\\n",
    "    -header -names ibdmix archaicseeker archie sprime vernot_2016 sankararaman_2014 sankararaman_2016_1 sankararaman_2016_2 skov_2020 steinruecken_2018 sarge > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_no_argweaver.bed\n",
    "\n",
    "# only keep chrom, start, stop\n",
    "!cut -f1,2,3 {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_no_argweaver.bed > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_no_argweaver_bed_only.bed\n",
    "# remove header\n",
    "!sed -e '1d' -i {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_no_argweaver_bed_only.bed\n",
    "# add chr prefix\n",
    "!sed -e 's/^/chr/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_no_argweaver_bed_only.bed > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/overlap_bed_only_no_argweaver_no_header_chr_prefix.bg\n",
    "\n",
    "# Create merged file. This file is generated from the bedgraphs and need to have a # in the first header line, which we did in the previous section\n",
    "!$BEDTOOLSDIR/bedtools merge -i {HOMEDIR}/introgression_methods/cleaned/introgression_tools/overlap_bed_only_no_argweaver_no_header_chr_prefix.bg \\\n",
    "    > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/merged_all_methods_no_argweaver.bed\n",
    "\n",
    "## Region level\n",
    "!{BEDTOOLSDIR}/bedtools intersect -a {HOMEDIR}/introgression_methods/cleaned/introgression_tools/overlap_bed_only_no_argweaver_no_header_chr_prefix.bg \\\n",
    "    -b {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/steinruecken_2018/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/schaefer_2021/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    -C -names ibdmix archaicseeker archie sprime vernot_2016 sankararaman_2014 sankararaman_2016_1 sankararaman_2016_2 skov_2020 steinruecken_2018 sarge \\\n",
    "   > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/region_intersect_for_boolean_no_argweaver.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ibdmix</th>\n",
       "      <th>archaicseeker</th>\n",
       "      <th>archie</th>\n",
       "      <th>sprime</th>\n",
       "      <th>vernot_2016</th>\n",
       "      <th>sankararaman_2014</th>\n",
       "      <th>sankararaman_2016_1</th>\n",
       "      <th>sankararaman_2016_2</th>\n",
       "      <th>skov_2020</th>\n",
       "      <th>steinruecken_2018</th>\n",
       "      <th>sarge</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>22746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364386</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>109687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364387</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>75368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364388</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>37507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364389</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364390</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364391 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ibdmix  archaicseeker  archie  sprime  vernot_2016  sankararaman_2014  \\\n",
       "0        False           True   False   False        False              False   \n",
       "1        False           True   False   False        False              False   \n",
       "2        False           True   False   False        False              False   \n",
       "3        False           True   False   False        False              False   \n",
       "4        False           True   False   False        False              False   \n",
       "...        ...            ...     ...     ...          ...                ...   \n",
       "364386   False          False   False   False        False              False   \n",
       "364387   False          False   False   False        False              False   \n",
       "364388   False          False   False   False        False              False   \n",
       "364389   False          False   False   False        False              False   \n",
       "364390   False          False   False   False        False              False   \n",
       "\n",
       "        sankararaman_2016_1  sankararaman_2016_2  skov_2020  \\\n",
       "0                     False                False      False   \n",
       "1                     False                False      False   \n",
       "2                     False                False      False   \n",
       "3                     False                False      False   \n",
       "4                     False                False      False   \n",
       "...                     ...                  ...        ...   \n",
       "364386                 True                False      False   \n",
       "364387                 True                False      False   \n",
       "364388                 True                False      False   \n",
       "364389                 True                False      False   \n",
       "364390                False                False      False   \n",
       "\n",
       "        steinruecken_2018  sarge  length  \n",
       "0                   False  False    4580  \n",
       "1                   False  False    5610  \n",
       "2                   False  False    4454  \n",
       "3                   False  False   22746  \n",
       "4                   False  False    9495  \n",
       "...                   ...    ...     ...  \n",
       "364386              False  False  109687  \n",
       "364387              False  False   75368  \n",
       "364388              False  False   37507  \n",
       "364389              False  False    3066  \n",
       "364390               True  False    1500  \n",
       "\n",
       "[364391 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BASE PAIRS LEVEL\n",
    "overlap = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_no_argweaver.bed', sep='\\t', low_memory=False)\n",
    "# replace 0s with NA\n",
    "overlap.replace(0, np.nan, inplace=True)\n",
    "# add length data\n",
    "overlap['length'] = overlap['end'] - overlap['start']\n",
    "overlap\n",
    "\n",
    "overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_no_argweaver.bed', sep='\\t', index=False)\n",
    "\n",
    "# save bed file version of all identified introgressed segments across methods\n",
    "overlap[['chrom', 'start', 'end']].to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/overlap_no_argweaver_bed_only.bed', sep='\\t', index=False)\n",
    "\n",
    "# first, convert dataframe to boolean values\n",
    "# List of columns to exclude\n",
    "exclude_columns = ['chrom', 'start', 'end', 'length']\n",
    "# Convert non-NA values to True and NA values to False, excluding specified columns\n",
    "boolean_overlap = overlap.copy()\n",
    "boolean_overlap[boolean_overlap.columns.difference(exclude_columns)] = boolean_overlap[boolean_overlap.columns.difference(exclude_columns)].applymap(lambda x: pd.notna(x))\n",
    "boolean_overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_overlap_no_argweaver.bed', sep='\\t', index=False)\n",
    "boolean_overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_overlap_no_argweaver_no_header.bed', sep='\\t', index=False, header=False)\n",
    "\n",
    "# drop chrom, start, end\n",
    "boolean_overlap = boolean_overlap.drop(['chrom', 'start', 'end'], axis=1)\n",
    "boolean_overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_overlap_no_argweaver_no_bed', sep='\\t', index=False)\n",
    "boolean_overlap\n",
    "\n",
    "## REGION LEVEL\n",
    "\n",
    "# # convert dataframe to boolean values\n",
    "# # read in regions overlap file that takes into account different loci by different algorithms\n",
    "# region_overlap = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/region_intersect_for_boolean_no_argweaver.bed', sep='\\t', low_memory=False, names=['chr', 'start', 'end', 'method', 'regions'])\n",
    "# # Group by chrom, start, end and \n",
    "# grouped = region_overlap.groupby(['chr', 'start', 'end', 'method']).sum().reset_index()\n",
    "# # Pivot table to turn methods -> columns\n",
    "# pivot_table = grouped.pivot_table(index=['chr', 'start', 'end'], columns='method', values='regions', fill_value=0)\n",
    "# # Reset index to convert to a flat DataFrame\n",
    "# pivot_table.reset_index(inplace=True)\n",
    "\n",
    "# # List of columns to exclude\n",
    "# exclude_columns = ['chr', 'start', 'end']\n",
    "# # Convert non-NA values to True and NA values to False, excluding specified columns\n",
    "# boolean_region_overlap = pivot_table.copy()\n",
    "# # replace 0s with NA\n",
    "# boolean_region_overlap.replace(0, np.nan, inplace=True)\n",
    "# # replace with boolean values\n",
    "# boolean_region_overlap[boolean_region_overlap.columns.difference(exclude_columns)] = boolean_region_overlap[boolean_region_overlap.columns.difference(exclude_columns)].applymap(lambda x: pd.notna(x))\n",
    "\n",
    "# boolean_region_overlap.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_region_overlap_no_argweaver.bed', sep='\\t', index=False)\n",
    "# # drop chrom, start, end\n",
    "# boolean_region_overlap_no_bed = boolean_region_overlap.drop(['chr', 'start', 'end'], axis=1)\n",
    "# boolean_region_overlap_no_bed.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_region_overlap_no_argweaver_no_bed', sep='\\t', index=False)\n",
    "# boolean_region_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### back to including argweaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chen_2020_pops = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/bedgraphs/union_merged.bg', sep='\\t')\n",
    "hubisz_2020_pops = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020/bedgraphs/union_merged.bg', sep='\\t')\n",
    "yuan_2021_pops = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/bedgraphs/union_merged.bg', sep='\\t')\n",
    "durvasula_2019_pops = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019/bedgraphs/union_merged.bg', sep='\\t', names=['chrom', 'start', 'end', 'CEU'])\n",
    "browning_2018_pops = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/bedgraphs/union_merged.bg', sep='\\t')\n",
    "vernot_2016_pops = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/bedgraphs/union_merged.bg', sep='\\t')\n",
    "sankararaman_2014_pops = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/bedgraphs/union_merged.bg', sep='\\t')\n",
    "sankararaman_2016_1_pops = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/bedgraphs/union_merged.bg', sep='\\t')\n",
    "sankararaman_2016_2_pops = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/bedgraphs/union_merged.bg', sep='\\t')\n",
    "steinruecken_2018_pops = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/steinruecken_2018/bedgraphs/union_merged.bg', sep='\\t')\n",
    "schaefer_2021_pops = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/schaefer_2021/bedgraphs/union_merged.bg', sep='\\t')\n",
    "skov_2020_pops = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/bedgraphs/union_merged_highest_score.bg', sep='\\t')\n",
    "skov_2020_pops.rename(columns={'highest_score': 'Icelandic'}, inplace=True)\n",
    "\n",
    "# Combine each separate file into one massive file\n",
    "all_methods_concat_pops = pd.concat([chen_2020_pops, hubisz_2020_pops, yuan_2021_pops, durvasula_2019_pops, browning_2018_pops, vernot_2016_pops, sankararaman_2014_pops, sankararaman_2016_1_pops, sankararaman_2016_2_pops, skov_2020_pops, steinruecken_2018_pops, schaefer_2021_pops], axis=0)\n",
    "all_methods_concat_pops = all_methods_concat_pops.fillna(0)\n",
    "all_methods_concat_pops.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops.bed', sep='\\t', index=False)\n",
    "\n",
    "# add '#' for header\n",
    "!sed -i '1,1s/^/#/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops.bed \n",
    "# collapse extra tabs\n",
    "!tr -s '\\t' '\\t' < {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops.bed > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops_header_tabs_collapse.bed\n",
    " # remove extra tabs and sort file\n",
    "!sed 's/\\t$//' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops_header_tabs_collapse.bed > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops_tab_removed.bed\n",
    "# remove header\n",
    "!sed -e '1d' -i {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops_tab_removed.bed\n",
    "!sort -k 1,1 -k2,2n {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops_tab_removed.bed > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops.bed\n",
    "# re-add header after sorting\n",
    "# Extract the first line from the source file\n",
    "!read -r first_line<$HOMEDIR/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops_tab_removed.bed\n",
    "# add header line\n",
    "!cp {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops.bed {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops_header.bed\n",
    "!sed -i '1s/^/#chrom\\tstart\\tend\\tEAS\\tEUR\\tAMR\\tSAS\\tAFR\\tAfrica\\tOceania\\tWestEurasia\\tCEU\\tASN\\tPNG\\tCentralAsia\\tIcelandic\\tCHBS\\tEastAsia\\tAfrica2\\tAmerica\\tCentralAsiaSiberia\\tSouthAsia\\n/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops_header.bed\n",
    "# had to hard code because sed was giving me such a hard time inserting first line from another file\n",
    "\n",
    "# remove intermediate files\n",
    "!rm {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops_header_tabs_collapse.bed \\\n",
    "{HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods_concat_pops_tab_removed.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chen_2020\n",
      "yuan_2021\n",
      "durvasula_2019\n",
      "steinruecken_2018\n",
      "skov_2020\n",
      "schaefer_2021\n",
      "hubisz_2020\n",
      "browning_2018\n",
      "vernot_2016\n",
      "sankararaman_2014\n",
      "sankararaman_2016_1\n",
      "sankararaman_2016_2\n"
     ]
    }
   ],
   "source": [
    "## region level - made sorted_union_merged_chr_prefix.bg\n",
    "methods_dirs = ['chen_2020', 'yuan_2021', 'durvasula_2019', 'steinruecken_2018', 'skov_2020', 'schaefer_2021', 'hubisz_2020', 'browning_2018', 'vernot_2016', 'sankararaman_2014', 'sankararaman_2016_1', 'sankararaman_2016_2']\n",
    "# Use bedtools complement to get the non-introgressed regions for each method\n",
    "for method in methods_dirs:\n",
    "    if 'sankararaman' in method:\n",
    "        method_name = method.replace('_', '', 1)\n",
    "    else:\n",
    "        method_name = method.replace('_', '')\n",
    "    # specify method dir\n",
    "    print(method)\n",
    "    method_dir = f'/introgression_methods/cleaned/introgression_tools/{method}'\n",
    "    # only keep chrom, start, stop\n",
    "    !cut -f1,2,3 {HOMEDIR}/{method_dir}/bedgraphs/union_merged.bg > {HOMEDIR}/{method_dir}/bedgraphs/union_merged_prefix.bg\n",
    "    # remove header\n",
    "    !sed -e '1d' -i {HOMEDIR}/{method_dir}/bedgraphs/union_merged_prefix.bg\n",
    "    # add chr prefix\n",
    "    !sed -e 's/^/chr/' {HOMEDIR}/{method_dir}/bedgraphs/union_merged_prefix.bg > {HOMEDIR}/{method_dir}/bedgraphs/union_merged_chr_prefix.bg\n",
    "    # sort both files\n",
    "    !sort -k1,1 /wynton/group/capra/data/hg19_fasta/2022-03-14/hg19.fa.fai > {HOMEDIR}/introgression_methods/data/sorted_hg19.fa.fai\n",
    "    !sort -k1,1 -k2,2n {HOMEDIR}/{method_dir}/bedgraphs/union_merged_chr_prefix.bg > {HOMEDIR}/{method_dir}/bedgraphs/sorted_union_merged_chr_prefix.bg\n",
    "    # run bedtools complement\n",
    "    # os.system(f'{BEDTOOLSDIR}/bedtools complement -L -i {HOMEDIR}/{method_dir}/bedgraphs/sorted_union_merged_chr_prefix.bg -g {HOMEDIR}/introgression_methods/data/sorted_hg19.fa.fai > {HOMEDIR}/{method_dir}/bedgraphs/complement.bg')\n",
    "    # # subtract gaps\n",
    "    # os.system(f'{BEDTOOLSDIR}/bedtools subtract -a {HOMEDIR}/{method_dir}/bedgraphs/complement.bg -b {HOMEDIR}/introgression_methods/data/hg19_gaps.txt > {HOMEDIR}/{method_dir}/complement_gaps_removed.bg')\n",
    "    # # keep desert regions only in filtered regions from the Nean genome - min filtering\n",
    "    # os.system(f'{BEDTOOLSDIR}/bedtools intersect -a {HOMEDIR}/{method_dir}/complement_gaps_removed.bg -b {HOMEDIR}/introgression_methods/data/altai_minimal_filters/AltaiNea.map35_50.MQ30.Cov.indels.TRF_chr_prefix.bed > {HOMEDIR}/{method_dir}/complement_gaps_removed_altai_filtered_min.bg')\n",
    "    # # keep desert regions only in filtered regions from the Nean genome - more extensive filtering from Altai\n",
    "    # os.system(f'{BEDTOOLSDIR}/bedtools intersect -a {HOMEDIR}/{method_dir}/complement_gaps_removed.bg -b {HOMEDIR}/introgression_methods/data/vindija_filtered_regions/altai_filtered_chr_prefix.bed > {HOMEDIR}/{method_dir}/complement_gaps_removed_altai_filtered.bg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a version with different universe: each unique combination of detected regions is a separate region\n",
    "## THIS ONE IS THE FINALIZED ONE\n",
    "!{BEDTOOLSDIR}/bedtools intersect -a {HOMEDIR}/introgression_methods/cleaned/introgression_tools/overlap_bed_only_no_header_chr_prefix.bg \\\n",
    "    -b {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/steinruecken_2018/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/schaefer_2021/bedgraphs/sorted_union_merged_chr_prefix.bg \\\n",
    "    -C -names ibdmix argweaverd archaicseeker archie sprime vernot_2016 sankararaman_2014 sankararaman_2016_1 sankararaman_2016_2 skov_2020 steinruecken_2018 sarge \\\n",
    "   > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/region_intersect_for_boolean.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate file with introgression across methods (no ARGWeaver-D), subsetted to a shared length across them, keeping the highest score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove header from \n",
    "methods_dirs = ['chen_2020', 'yuan_2021', 'durvasula_2019', 'steinruecken_2018', 'skov_2020', 'schaefer_2021', 'hubisz_2020', 'browning_2018', 'vernot_2016', 'sankararaman_2014', 'sankararaman_2016_1', 'sankararaman_2016_2']\n",
    "# Use bedtools complement to get the non-introgressed regions for each method\n",
    "for method in methods_dirs:\n",
    "    method_dir = f'/introgression_methods/cleaned/introgression_tools/{method}'\n",
    "    with open(f'{HOMEDIR}/{method_dir}/bedgraphs/union_merged_highest_score.bg', 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        if 'chrom' in first_line.lower():\n",
    "            !sed -e '1d' -i {HOMEDIR}/{method_dir}/bedgraphs/union_merged_highest_score.bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chen_2020': 1141804104, 'yuan_2021': 1141804104, 'durvasula_2019': 1141804104, 'steinruecken_2018': 1141804104, 'skov_2020': 1141804104, 'schaefer_2021': 1141804104, 'hubisz_2020': 1141804104, 'browning_2018': 1141804104, 'vernot_2016': 1141804104, 'sankararaman_2014': 1141804104, 'sankararaman_2016_1': 1141804104, 'sankararaman_2016_2': 1141804104} 1141804104\n"
     ]
    }
   ],
   "source": [
    "# empty dict to hold proportions\n",
    "proportions = {}\n",
    "for method in methods_dirs:\n",
    "    df = pd.read_csv(f'{HOMEDIR}/{method_dir}/bedgraphs/union_merged_highest_score.bg', sep='\\t', low_memory=False, names=['chrom', 'Start', 'End', 'Score'])\n",
    "    # compute length\n",
    "    df['length'] = df['End'] - df['Start']\n",
    "    # compute the minumum amount of introgression across people\n",
    "    min_sum = df['length'].sum().min()\n",
    "    # add the minimum amount to the dict\n",
    "    proportions[method] = min_sum\n",
    "# get the smallest value to use as length_threshold\n",
    "length_threshold = min(proportions.values())\n",
    "print(proportions, length_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_overlap_shared_length_highest_scores = tools.generate_overlap(input_files_string=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/bedgraphs/union_merged_highest_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/bedgraphs/union_merged_highest_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019/bedgraphs/union_merged_highest_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/bedgraphs/union_merged_highest_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/bedgraphs/union_merged_highest_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/bedgraphs/union_merged_highest_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/bedgraphs/union_merged_highest_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/bedgraphs/union_merged_highest_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/bedgraphs/union_merged_highest_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/steinruecken_2018/bedgraphs/union_merged_highest_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/schaefer_2021/bedgraphs/union_merged_highest_score.bg',\n",
    "                                           input_files_names_str='ibdmix archaicseeker archie sprime vernot_2016 sankararaman_2014 sankararaman_2016_1 sankararaman_2016_2 skov_2020 steinruecken_2018 sarge',\n",
    "                                           output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_overlap_shared_length_highest_scores.bed',\n",
    "                                           boolean=True, autosomes=True, length_threshold=719560342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also a version with EUR regions only\n",
    "boolean_overlap_shared_length_highest_scores_EUR = tools.generate_overlap(input_files_string=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/bedgraphs/union_merged_highest_score.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/bedgraphs/merged_EUR.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019/bedgraphs/merged_EUR.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/bedgraphs/merged_EUR.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/bedgraphs/merged_EUR.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/bedgraphs/merged_EUR.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/bedgraphs/merged_EUR.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/bedgraphs/merged_EUR.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/bedgraphs/merged_Icelandic.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/steinruecken_2018/bedgraphs/merged_CEU.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/schaefer_2021/bedgraphs/merged_WestEurasia.bg',\n",
    "                                           input_files_names_str='ibdmix archaicseeker archie sprime vernot_2016 sankararaman_2014 sankararaman_2016_1 sankararaman_2016_2 skov_2020 steinruecken_2018 sarge',\n",
    "                                           output_file_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/boolean_overlap_shared_length_highest_scores_EUR.bed',\n",
    "                                           boolean=True, autosomes=True, length_threshold=719560342)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate previously computed deserts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available deserts:\n",
    "- Chen 2020 (in Table S8)\n",
    "- Skov 2020 (Supplementary Dataset 4, 41586_2020_2225_MOESM6_ESM, \"This dataset reports the chrom, desert number, start, mean windows called as archaic, number of called and total number of genes.\")\n",
    "- Vernot 2016 (In Chen 2020 Table S8)\n",
    "- Sankararaman 2016 (previously emailed Dr. Sankararaman)\n",
    "- Yuan 2021 (Supplementary Data 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## manually write .bed files for Chen 2020 and Vernot deserts:\n",
    "chen_2020_deserts = {\n",
    "    'chrom': [1, 3, 7, 8],\n",
    "    'start': [105400000, 74100000, 106200000, 49400000],\n",
    "    'end': [120600000, 89300000, 123200000, 66500000]\n",
    "}\n",
    "\n",
    "chen_2020_deserts_df = pd.DataFrame(chen_2020_deserts)\n",
    "chen_2020_deserts_df.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/original_deserts.bg', header=False, sep='\\t', index=False)\n",
    "\n",
    "# save version with chr prefix\n",
    "chen_2020_deserts_df['chrom'] = 'chr' + chen_2020_deserts_df['chrom'].astype(str)\n",
    "chen_2020_deserts_df.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/original_deserts_chr_prefix.bg', header=False, sep='\\t', index=False)\n",
    "\n",
    "vernot_2016_deserts = {\n",
    "    'chrom': [1, 2, 3, 7, 8, 18],\n",
    "    'start': [102200000, 201100000, 76500000, 106300000, 53900000, 25000000],\n",
    "    'end': [114900000, 211500000, 90500000, 124700000, 66000000, 41800000]\n",
    "}\n",
    "\n",
    "vernot_2016_deserts_df = pd.DataFrame(vernot_2016_deserts)\n",
    "vernot_2016_deserts_df.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/original_deserts.bg', header=False, sep='\\t', index=False)\n",
    "\n",
    "# save version with chr prefix\n",
    "vernot_2016_deserts_df['chrom'] = 'chr' + vernot_2016_deserts_df['chrom'].astype(str)\n",
    "vernot_2016_deserts_df.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/original_deserts_chr_prefix.bg', header=False, sep='\\t', index=False)\n",
    "\n",
    "# load in ArchaicSeeker2 deserts file (supplementary figure 6)\n",
    "yuan_2021_deserts=pd.read_excel(f'{HOMEDIR}/introgression_methods/data/archaicseeker2.0_yuan_et_al_2021/41467_2021_26503_MOESM9_ESM.xlsx')\n",
    "# save only as bed file with chrom, start, end\n",
    "yuan_2021_deserts[['Chromosome', 'Start (bp)', 'End (bp)']].to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/original_deserts.bg', header=False, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sankararaman deserts are already in a cleaned .bed format, copy over to cleaned directory\n",
    "! cp {HOMEDIR}/introgression_methods/data/sankararaman_2016/deserts/sankararaman16_nean-deserts_hg19.bed {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/original_deserts_chr_prefix.bg\n",
    "\n",
    "sankararaman_2016_deserts = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/original_deserts_chr_prefix.bg', sep='\\t', names=['chrom', 'start', 'end'])\n",
    "sankararaman_2016_deserts['chrom'] = sankararaman_2016_deserts['chrom'].str.replace('chr', '')\n",
    "sankararaman_2016_deserts.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/original_deserts.bg', header=False, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in Skov 2020 deserts file -- Need to liftover from hg38 to hg19\n",
    "skov_2020_deserts = pd.read_csv(f'{HOMEDIR}/introgression_methods/data/skov_2020/41586_2020_2225_MOESM6_ESM.txt', sep='\\t')\n",
    "skov_2020_deserts = skov_2020_deserts[['chrom', 'start', 'end']]\n",
    "skov_2020_deserts['start'] = np.int64(skov_2020_deserts['start'])\n",
    "skov_2020_deserts['end'] = np.int64(skov_2020_deserts['end'])\n",
    "# add chr prefix\n",
    "skov_2020_deserts['chrom'] = 'chr' + skov_2020_deserts['chrom']\n",
    "#skov_2020_deserts.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/original_deserts_hg38.bg', header=False, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skov_2020_deserts = pd.read_csv(f'{HOMEDIR}/introgression_methods/data/skov_2020/41586_2020_2225_MOESM6_ESM.txt', sep='\\t')\n",
    "\n",
    "np.sum(skov_2020_deserts.mean_called)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading liftover chains\n",
      "Mapping coordinates\n"
     ]
    }
   ],
   "source": [
    "# run liftover\n",
    "!/wynton/group/capra/bin/liftOver/liftOver \\\n",
    "{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/original_deserts_hg38.bg \\\n",
    "{HOMEDIR}/introgression_methods/data/skov_2020/hg38_to_hg19_liftover/hg38ToHg19.over.chain.gz \\\n",
    "{HOMEDIR}/introgression_methods/data/skov_2020/hg38_to_hg19_liftover/original_deserts_hg19.bed \\\n",
    "{HOMEDIR}/introgression_methods/data/skov_2020/hg38_to_hg19_liftover/deserts_unlifted.bed\n",
    "\n",
    "# copy final lifted file to cleaned directory\n",
    "!cp {HOMEDIR}/introgression_methods/data/skov_2020/hg38_to_hg19_liftover/original_deserts_hg19.bed {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/original_deserts_chr_prefix.bg\n",
    "# remove 'chr' prefix in cleaned file\n",
    "skov_2020_deserts = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/original_deserts_chr_prefix.bg', sep='\\t', names=['chrom', 'start', 'end'])\n",
    "skov_2020_deserts['chrom'] = skov_2020_deserts['chrom'].str.replace('chr', '')\n",
    "skov_2020_deserts.to_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/original_deserts.bg', header=False, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After LiftOver, we went from 281 desert regions in hg38 -> 268 desert regions in hg19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined desert file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the bg file\n",
    "bedops_dir='/wynton/home/cbi/shared/software/CBI/bedops-2.4.41/bin/'\n",
    "!{bedops_dir}/sort-bed --unique {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/original_deserts.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/sorted_original_deserts.bg\n",
    "!{bedops_dir}/sort-bed --unique {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/original_deserts.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/sorted_original_deserts.bg\n",
    "!{bedops_dir}/sort-bed --unique {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/original_deserts.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/sorted_original_deserts.bg\n",
    "!{bedops_dir}/sort-bed --unique {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/original_deserts.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/sorted_original_deserts.bg\n",
    "!{bedops_dir}/sort-bed --unique {HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/original_deserts.bg > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/sorted_original_deserts.bg\n",
    "\n",
    "# add 1s\n",
    "!sed -i 's/$/&\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/sorted_original_deserts.bg\n",
    "!sed -i 's/$/&\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/sorted_original_deserts.bg\n",
    "!sed -i 's/$/&\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/sorted_original_deserts.bg\n",
    "!sed -i 's/$/&\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/sorted_original_deserts.bg\n",
    "!sed -i 's/$/&\\t1/' {HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/sorted_original_deserts.bg\n",
    "\n",
    "# generate a combined file\n",
    "!$BEDTOOLSDIR/bedtools unionbedg -i {HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/sorted_original_deserts.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/sorted_original_deserts.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/sorted_original_deserts.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/sorted_original_deserts.bg \\\n",
    "    {HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/sorted_original_deserts.bg \\\n",
    "    -header -names sankararaman_2016_1 skov_2020 chen_2020 vernot_2016 yuan_2021 > {HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_deserts.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Generate files for rGREAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I generate the overall Neanderhal introgression background file, where any region considered introgressed, regardless of method, is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background file containing all regions that were considered introgressed (all_methods.bed file that we previously created)\n",
    "# need to remove the last columns\n",
    "# directory to store data\n",
    "!mkdir {HOMEDIR}/introgression_methods/data/GREAT_analysis\n",
    "!mkdir {HOMEDIR}/introgression_methods/data/GREAT_analysis/input\n",
    "overlap = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods.bed', sep='\\t', low_memory=False)\n",
    "nean_background = overlap[['chrom', 'start', 'end']]\n",
    "nean_background['chrom'] = 'chr' + nean_background['chrom'].astype(str)\n",
    "nean_background.to_csv(f'{HOMEDIR}/introgression_methods/data/GREAT_analysis/nean_background.bed', sep='\\t', index=False, header=False)\n",
    "# merge overlapping entries that were previously separated due to counting the overlap between different sets of methods, now we are not worried about a score associated with each introgressed region\n",
    "!$BEDTOOLSDIR/bedtools merge -i {HOMEDIR}/introgression_methods/data/GREAT_analysis/nean_background.bed > {HOMEDIR}/introgression_methods/data/GREAT_analysis/nean_background_merged.bed\n",
    "# remove unmerged file\n",
    "!rm {HOMEDIR}/introgression_methods/data/GREAT_analysis/nean_background.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will generate test files for different introgressed regions I want to test, and background files containing all regions that contain introgression.\n",
    "\n",
    "Here is a function to generate non-overlapping bed files with each method as the test set, and all detected Neanderthal introgressed regions as the background set (the background set must contain identical test set entries, plus other regions that are part of the background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GREAT requires background file to contain exactly the same entries as the test, plus extra background regions as separate entries\n",
    "# subtract all nean introgression regions - test introgression set to get extra introgressed regions\n",
    "def GREAT_background_files(method_name=None, test_df=None, test_df_path=None):\n",
    "    if test_df_path is not None:\n",
    "        df = pd.read_csv(f'{test_df_path}', sep='\\t', low_memory=False, names=['chrom', 'start', 'end'])\n",
    "    elif test_df is not None:\n",
    "        df = test_df\n",
    "    # add chr prefix if the column contains integers\n",
    "    if df['chrom'].dtype != 'str':\n",
    "        df['chrom'] = 'chr' + df['chrom'].astype(str)\n",
    "    test_bed = df[['chrom', 'start', 'end']]\n",
    "    test_bed.to_csv(f'{HOMEDIR}/introgression_methods/data/GREAT_analysis/{method_name}_GREAT.bed', sep='\\t', index=False, header=False)\n",
    "    !$BEDTOOLSDIR/bedtools merge -i {HOMEDIR}/introgression_methods/data/GREAT_analysis/{method_name}_GREAT.bed > {HOMEDIR}/introgression_methods/data/GREAT_analysis/input/{method_name}_GREAT_merged.bed\n",
    "    # remove unmerged file\n",
    "    # subtract test regions from full nean background file \n",
    "    !$BEDTOOLSDIR/bedtools subtract -a {HOMEDIR}/introgression_methods/data/GREAT_analysis/nean_background_merged.bed \\\n",
    "    -b {HOMEDIR}/introgression_methods/data/GREAT_analysis/input/{method_name}_GREAT_merged.bed > {HOMEDIR}/introgression_methods/data/GREAT_analysis/background_sub_{method_name}.bed\n",
    "    # append original test regions to nean introgressed regions (subtracted test from full nean background file)\n",
    "    !cat {HOMEDIR}/introgression_methods/data/GREAT_analysis/input/{method_name}_GREAT_merged.bed {HOMEDIR}/introgression_methods/data/GREAT_analysis/background_sub_{method_name}.bed > {HOMEDIR}/introgression_methods/data/GREAT_analysis/{method_name}_background.bed\n",
    "    # re-sort regions\n",
    "    !$BEDTOOLSDIR/bedtools sort -i {HOMEDIR}/introgression_methods/data/GREAT_analysis/{method_name}_background.bed > {HOMEDIR}/introgression_methods/data/GREAT_analysis/input/{method_name}_background_GREAT.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREAT_background_files(test_df_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/chen_2020/bedgraphs/union_merged_prefix.bg', method_name='ibdmix')\n",
    "GREAT_background_files(test_df_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/hubisz_2020/bedgraphs/union_merged_prefix.bg', method_name='argweaverd')\n",
    "GREAT_background_files(test_df_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/yuan_2021/bedgraphs/union_merged_prefix.bg', method_name='archaicseeker')\n",
    "GREAT_background_files(test_df_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/durvasula_2019/bedgraphs/union_merged_prefix.bg', method_name='archie')\n",
    "GREAT_background_files(test_df_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/browning_2018/bedgraphs/union_merged_prefix.bg', method_name='sprime')\n",
    "GREAT_background_files(test_df_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/vernot_2016/bedgraphs/union_merged_prefix.bg', method_name='vernot2016')\n",
    "GREAT_background_files(test_df_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2014/bedgraphs/union_merged_prefix.bg', method_name='sankararaman2014')\n",
    "GREAT_background_files(test_df_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_1/bedgraphs/union_merged_prefix.bg', method_name='sankararaman2016_1')\n",
    "GREAT_background_files(test_df_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/sankararaman_2016_2/bedgraphs/union_merged_prefix.bg', method_name='sankararaman2016_2')\n",
    "GREAT_background_files(test_df_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/skov_2020/bedgraphs/union_merged_prefix.bg', method_name='skov2020')\n",
    "GREAT_background_files(test_df_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/steinruecken_2018/bedgraphs/union_merged_prefix.bg', method_name='steinruecken2018')\n",
    "GREAT_background_files(test_df_path=f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/schaefer_2021/bedgraphs/union_merged_prefix.bg', method_name='sarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate files for GREAT input\n",
    "GREAT_background_files(sarge, 'sarge') # no enrichments under default parameters\n",
    "GREAT_background_files(ibdmix, 'ibdmix') # no enrichments under default parameters\n",
    "GREAT_background_files(archaicseeker, 'archaicseeker') # no enrichments under default parameters\n",
    "GREAT_background_files(sankararaman_2016_1, 'sankararaman2016_1') # no enrichments under default parameters\n",
    "GREAT_background_files(sankararaman_2016_2, 'sankararaman2016_2') # no enrichments under default parameters\n",
    "GREAT_background_files(sprime, 'sprime') # no enrichments under default parameters\n",
    "GREAT_background_files(vernot_2016, 'vernot2016') # no enrichments under default parameters\n",
    "GREAT_background_files(argweaverd, 'argweaverd') # HAS ENRICHMENTS: GPHN is top gene, scaffolding molecule at inhibitory neuron synapses\n",
    "GREAT_background_files(sankararaman_2014, 'sankararaman2014') # no enrichements under default parameters\n",
    "GREAT_background_files(skov_2020, 'skov2020') # HAS ENRICHMENTS: axonal growth cone, amino acid binding, pancreas size (in mouse)\n",
    "GREAT_background_files(steinruecken_2018, 'steinruecken2018') # no enrichments under default parameters\n",
    "GREAT_background_files(archie, 'archie') # HAS ENRICHMENTS: hypoxia, respiratory birse, cell migration in hindbrain :) and lots of other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to test if regions overlapping across all 12 methods have specific genomic elements compared to the background. Additionally, I will repeat the test but excluding ArchIE and ARGweaver-D which have small test sets to look at regions overlapping over the remaining 10 methods (high-confidence Neanderthal introgressed methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bed file with all 12 methods overlapping\n",
    "overlap_12_methods = overlap.dropna()\n",
    "overlap_12_methods = overlap_12_methods[['chrom', 'start', 'end']]\n",
    "\n",
    "GREAT_background_files('overlap_12_methods', test_df=overlap_12_methods) # HAS ENRICHMENTS: lots of telomere processes, ATPase activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_10_methods = overlap.drop(columns=['archie', 'argweaverd'])\n",
    "overlap_10_methods = overlap_10_methods.dropna()\n",
    "overlap_10_methods_great = overlap_10_methods[['chrom', 'start', 'end']]\n",
    "\n",
    "GREAT_background_files('overlap_10_no_archie_argweaver', test_df=overlap_10_methods_great) # no enrichments under default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also generate GREAT files where at least 10 methods support a region (including ArchIE and ARGweaver-D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466317072"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap = pd.read_csv(f'{HOMEDIR}/introgression_methods/cleaned/introgression_tools/all_methods.bed', sep='\\t', low_memory=False)\n",
    "overlap.replace(0, np.nan, inplace=True)\n",
    "# List of columns to exclude to make boolean\n",
    "exclude_columns = ['chrom', 'start', 'end', 'length']\n",
    "overlap[overlap.columns.difference(exclude_columns)] = overlap[overlap.columns.difference(exclude_columns)].apply(lambda x: pd.notna(x))\n",
    "\n",
    "# methods cols to check for boolean\n",
    "columns_to_check = [item for item in overlap.columns if item not in exclude_columns]\n",
    "\n",
    "# keep rows with at least 10 TRUE values\n",
    "mask = overlap[columns_to_check].fillna(False).astype(bool).sum(axis=1) >= 10\n",
    "\n",
    "# filter the original DataFrame based on the mask\n",
    "overlap_10_more = overlap[mask]\n",
    "overlap_10_more # number matches the num support plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREAT_background_files('overlap_10_more_methods', test_df=overlap_10_more)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
